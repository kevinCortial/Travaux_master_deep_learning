{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Deep Learning Exercice2 Kévin CORTIAL et William GIRALDO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairie utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "from keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.scorer import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des bases de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/CORTIAL/Documents/Universite/M2/Deep_learning/dm/simulations.csv\", header=None, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.095695</td>\n",
       "      <td>52.411846</td>\n",
       "      <td>46.665506</td>\n",
       "      <td>0.456857</td>\n",
       "      <td>0.498308</td>\n",
       "      <td>0.462544</td>\n",
       "      <td>0.569043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.174525</td>\n",
       "      <td>0.118662</td>\n",
       "      <td>55.276823</td>\n",
       "      <td>47.986155</td>\n",
       "      <td>0.467475</td>\n",
       "      <td>0.473695</td>\n",
       "      <td>0.324767</td>\n",
       "      <td>0.457710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.130938</td>\n",
       "      <td>0.087290</td>\n",
       "      <td>57.602196</td>\n",
       "      <td>49.147401</td>\n",
       "      <td>0.583165</td>\n",
       "      <td>0.383176</td>\n",
       "      <td>0.428328</td>\n",
       "      <td>0.528848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135862</td>\n",
       "      <td>0.076790</td>\n",
       "      <td>63.949029</td>\n",
       "      <td>53.460064</td>\n",
       "      <td>0.724916</td>\n",
       "      <td>0.255798</td>\n",
       "      <td>0.386492</td>\n",
       "      <td>0.497157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.116687</td>\n",
       "      <td>0.083390</td>\n",
       "      <td>54.182646</td>\n",
       "      <td>47.427364</td>\n",
       "      <td>0.483154</td>\n",
       "      <td>0.483668</td>\n",
       "      <td>0.515468</td>\n",
       "      <td>0.622133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>0.191387</td>\n",
       "      <td>0.133257</td>\n",
       "      <td>54.748808</td>\n",
       "      <td>47.904946</td>\n",
       "      <td>0.376072</td>\n",
       "      <td>0.539880</td>\n",
       "      <td>0.329150</td>\n",
       "      <td>0.478433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>0.194837</td>\n",
       "      <td>0.129781</td>\n",
       "      <td>56.075191</td>\n",
       "      <td>48.558083</td>\n",
       "      <td>0.453328</td>\n",
       "      <td>0.475563</td>\n",
       "      <td>0.269954</td>\n",
       "      <td>0.413738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.069681</td>\n",
       "      <td>59.996400</td>\n",
       "      <td>50.625436</td>\n",
       "      <td>0.753639</td>\n",
       "      <td>0.231736</td>\n",
       "      <td>0.462265</td>\n",
       "      <td>0.544195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>0.140025</td>\n",
       "      <td>0.098967</td>\n",
       "      <td>53.189520</td>\n",
       "      <td>46.879180</td>\n",
       "      <td>0.455035</td>\n",
       "      <td>0.495934</td>\n",
       "      <td>0.446288</td>\n",
       "      <td>0.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>0.154363</td>\n",
       "      <td>0.080843</td>\n",
       "      <td>66.285772</td>\n",
       "      <td>55.199387</td>\n",
       "      <td>0.716676</td>\n",
       "      <td>0.259410</td>\n",
       "      <td>0.331365</td>\n",
       "      <td>0.465443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            8         9          10         11        12        13        14  \\\n",
       "0     0.134850  0.095695  52.411846  46.665506  0.456857  0.498308  0.462544   \n",
       "1     0.174525  0.118662  55.276823  47.986155  0.467475  0.473695  0.324767   \n",
       "2     0.130938  0.087290  57.602196  49.147401  0.583165  0.383176  0.428328   \n",
       "3     0.135862  0.076790  63.949029  53.460064  0.724916  0.255798  0.386492   \n",
       "4     0.116687  0.083390  54.182646  47.427364  0.483154  0.483668  0.515468   \n",
       "...        ...       ...        ...        ...       ...       ...       ...   \n",
       "9995  0.191387  0.133257  54.748808  47.904946  0.376072  0.539880  0.329150   \n",
       "9996  0.194837  0.129781  56.075191  48.558083  0.453328  0.475563  0.269954   \n",
       "9997  0.111100  0.069681  59.996400  50.625436  0.753639  0.231736  0.462265   \n",
       "9998  0.140025  0.098967  53.189520  46.879180  0.455035  0.495934  0.446288   \n",
       "9999  0.154363  0.080843  66.285772  55.199387  0.716676  0.259410  0.331365   \n",
       "\n",
       "            15  \n",
       "0     0.569043  \n",
       "1     0.457710  \n",
       "2     0.528848  \n",
       "3     0.497157  \n",
       "4     0.622133  \n",
       "...        ...  \n",
       "9995  0.478433  \n",
       "9996  0.413738  \n",
       "9997  0.544195  \n",
       "9998  0.563200  \n",
       "9999  0.465443  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:,8:16]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.120964</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.618706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.157933</td>\n",
       "      <td>0.077691</td>\n",
       "      <td>0.614342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.064752</td>\n",
       "      <td>0.526321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081305</td>\n",
       "      <td>0.104735</td>\n",
       "      <td>0.351984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.082942</td>\n",
       "      <td>0.032711</td>\n",
       "      <td>0.634617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>0.149131</td>\n",
       "      <td>0.082215</td>\n",
       "      <td>0.741594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>0.162341</td>\n",
       "      <td>0.099584</td>\n",
       "      <td>0.632764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>0.065609</td>\n",
       "      <td>0.285652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>0.127049</td>\n",
       "      <td>0.041564</td>\n",
       "      <td>0.619351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>0.060227</td>\n",
       "      <td>0.132472</td>\n",
       "      <td>0.357735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2\n",
       "0     0.120964  0.035158  0.618706\n",
       "1     0.157933  0.077691  0.614342\n",
       "2     0.104400  0.064752  0.526321\n",
       "3     0.081305  0.104735  0.351984\n",
       "4     0.082942  0.032711  0.634617\n",
       "...        ...       ...       ...\n",
       "9995  0.149131  0.082215  0.741594\n",
       "9996  0.162341  0.099584  0.632764\n",
       "9997  0.128866  0.065609  0.285652\n",
       "9998  0.127049  0.041564  0.619351\n",
       "9999  0.060227  0.132472  0.357735\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = data.iloc[:,0:3]\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble des données comprend 10 000 données. A partir des 8 variables de la base de données X, nous cherchons à prédire 3 variables réelles qui sont dans la base de données Y.\n",
    "\n",
    "On décompose aléatoirement les données en 8000 individus pour l’ensemble d’apprentissage et 2000 données pour tester notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation de 10 000 individus en un dataset train de 8 000 lignes et un dataset test de 2 000 lignes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X),np.array(Y), test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle\n",
    "\n",
    "Afin de construire notre réseau de neurones, nous avons tout d'abord choisit les paramètres suivants :\n",
    "- Une initialisation des poids initiaux 'uniform' entre -0.05 et 0.05.\n",
    "- Une fonction d'activation 'relu' pour les couches cachées.\n",
    "- Une fonction d'activation 'linear' pour la sortie du réseau, comme il est demandé dans l'énoncé.\n",
    "- l’algorithme 'adam' qui est généralement le plus pertinent pour la descente du gradient.\n",
    "- la fonction de perte logarithmique 'mean_absolute_percentage_error'\n",
    "- Nous mesurons les performances de notre réseau en calculant MAPE\n",
    "\n",
    "Ces paramètres pourront être modifiés dans la suite du notebook\n",
    "\n",
    "Afin de déterminer la profondeur optimale de notre réseau, nous avons testé \"à la main\" les performances des modèles ayant (1, 2, ..., 5) couches cachées avec un nombre de neurone arbitraires. En comparant les MAPE des 5 modèles, nous remarquons que le modèle avec 2 couches cachées est celui qui obtient le meilleur score.\n",
    "\n",
    "Par la suite, nous avons souhaité déterminer la combinaison optimale du nombre de neurones dans le réseau. Pour cela, dans une boucle for, nous avons fait varier le nombre de neuronnes des 2 couches \"i\" et \"j\" entre 10 et 30. 10 étant pour nous le minimum puisque nous avons 8 variables en entrée du modèle. Et 30 car nous ne souhaitions pas avoir un modèle trop complexe pour l'hyperparamétrisation à la fin du notebook. En comparant les MAPE des (20*20 = 400) modèles, le réseau qui avait le meilleur MAPE était celui avec 25 neurones dans la première couche et 20 dans la deuxième couche cachée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25)                225       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                520       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 808\n",
      "Trainable params: 808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() #Initialisation modèle\n",
    "\n",
    "model.add(Dense(25, input_dim=8, \n",
    "                kernel_initializer='uniform', \n",
    "                activation='relu'))\n",
    "\n",
    "#deuxième couche : 20 neurones\n",
    "model.add(Dense(20, kernel_initializer='uniform', \n",
    "                activation='relu'))\n",
    "\n",
    "model.add(Dense(3, kernel_initializer='uniform', activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_absolute_percentage_error', \n",
    "              optimizer='adam', \n",
    "              metrics = ['mape']) #une ou plrs fonction pour évaluer le modèle)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/250\n",
      " - 5s - loss: 45.0591 - mape: 45.0591 - val_loss: 34.0547 - val_mape: 34.0547\n",
      "Epoch 2/250\n",
      " - 2s - loss: 28.5152 - mape: 28.5152 - val_loss: 21.4015 - val_mape: 21.4015\n",
      "Epoch 3/250\n",
      " - 2s - loss: 21.1828 - mape: 21.1828 - val_loss: 23.1679 - val_mape: 23.1679\n",
      "Epoch 4/250\n",
      " - 2s - loss: 19.6820 - mape: 19.6820 - val_loss: 19.1047 - val_mape: 19.1047\n",
      "Epoch 5/250\n",
      " - 1s - loss: 19.0013 - mape: 19.0013 - val_loss: 18.3261 - val_mape: 18.3261\n",
      "Epoch 6/250\n",
      " - 2s - loss: 18.4769 - mape: 18.4770 - val_loss: 18.7101 - val_mape: 18.7101\n",
      "Epoch 7/250\n",
      " - 1s - loss: 17.7037 - mape: 17.7037 - val_loss: 15.6826 - val_mape: 15.6826\n",
      "Epoch 8/250\n",
      " - 1s - loss: 15.6322 - mape: 15.6322 - val_loss: 13.3128 - val_mape: 13.3128\n",
      "Epoch 9/250\n",
      " - 1s - loss: 13.4546 - mape: 13.4546 - val_loss: 11.4603 - val_mape: 11.4603\n",
      "Epoch 10/250\n",
      " - 1s - loss: 11.1107 - mape: 11.1107 - val_loss: 9.0701 - val_mape: 9.0701\n",
      "Epoch 11/250\n",
      " - 2s - loss: 9.9161 - mape: 9.9161 - val_loss: 8.9206 - val_mape: 8.9206\n",
      "Epoch 12/250\n",
      " - 2s - loss: 9.6392 - mape: 9.6392 - val_loss: 8.1760 - val_mape: 8.1760\n",
      "Epoch 13/250\n",
      " - 2s - loss: 9.1073 - mape: 9.1073 - val_loss: 9.1631 - val_mape: 9.1631\n",
      "Epoch 14/250\n",
      " - 2s - loss: 9.1346 - mape: 9.1346 - val_loss: 7.1824 - val_mape: 7.1824\n",
      "Epoch 15/250\n",
      " - 2s - loss: 8.5979 - mape: 8.5979 - val_loss: 8.4917 - val_mape: 8.4917\n",
      "Epoch 16/250\n",
      " - 2s - loss: 8.8359 - mape: 8.8359 - val_loss: 7.8889 - val_mape: 7.8889\n",
      "Epoch 17/250\n",
      " - 1s - loss: 8.2549 - mape: 8.2549 - val_loss: 9.8198 - val_mape: 9.8198\n",
      "Epoch 18/250\n",
      " - 2s - loss: 8.3472 - mape: 8.3472 - val_loss: 7.6207 - val_mape: 7.6207\n",
      "Epoch 19/250\n",
      " - 1s - loss: 7.8167 - mape: 7.8167 - val_loss: 8.5386 - val_mape: 8.5386\n",
      "Epoch 20/250\n",
      " - 1s - loss: 7.9236 - mape: 7.9236 - val_loss: 8.9252 - val_mape: 8.9252\n",
      "Epoch 21/250\n",
      " - 3s - loss: 7.8014 - mape: 7.8014 - val_loss: 8.4067 - val_mape: 8.4067\n",
      "Epoch 22/250\n",
      " - 2s - loss: 7.7436 - mape: 7.7436 - val_loss: 11.0394 - val_mape: 11.0394\n",
      "Epoch 23/250\n",
      " - 2s - loss: 7.9219 - mape: 7.9219 - val_loss: 8.8195 - val_mape: 8.8195\n",
      "Epoch 24/250\n",
      " - 2s - loss: 7.6084 - mape: 7.6084 - val_loss: 6.1216 - val_mape: 6.1216\n",
      "Epoch 25/250\n",
      " - 2s - loss: 7.7818 - mape: 7.7818 - val_loss: 7.2132 - val_mape: 7.2132\n",
      "Epoch 26/250\n",
      " - 2s - loss: 7.3783 - mape: 7.3783 - val_loss: 5.8500 - val_mape: 5.8500\n",
      "Epoch 27/250\n",
      " - 1s - loss: 7.3383 - mape: 7.3383 - val_loss: 7.1647 - val_mape: 7.1647\n",
      "Epoch 28/250\n",
      " - 1s - loss: 7.2935 - mape: 7.2935 - val_loss: 9.1005 - val_mape: 9.1005\n",
      "Epoch 29/250\n",
      " - 2s - loss: 7.3150 - mape: 7.3150 - val_loss: 6.9201 - val_mape: 6.9201\n",
      "Epoch 30/250\n",
      " - 2s - loss: 7.2893 - mape: 7.2893 - val_loss: 7.3026 - val_mape: 7.3026\n",
      "Epoch 31/250\n",
      " - 2s - loss: 6.9994 - mape: 6.9994 - val_loss: 11.6034 - val_mape: 11.6034\n",
      "Epoch 32/250\n",
      " - 1s - loss: 7.1891 - mape: 7.1891 - val_loss: 7.9390 - val_mape: 7.9390\n",
      "Epoch 33/250\n",
      " - 1s - loss: 6.7780 - mape: 6.7780 - val_loss: 6.3274 - val_mape: 6.3274\n",
      "Epoch 34/250\n",
      " - 2s - loss: 6.8089 - mape: 6.8089 - val_loss: 7.5853 - val_mape: 7.5853\n",
      "Epoch 35/250\n",
      " - 2s - loss: 6.6409 - mape: 6.6409 - val_loss: 6.9653 - val_mape: 6.9653\n",
      "Epoch 36/250\n",
      " - 2s - loss: 6.7970 - mape: 6.7970 - val_loss: 8.9045 - val_mape: 8.9045\n",
      "Epoch 37/250\n",
      " - 2s - loss: 6.8853 - mape: 6.8853 - val_loss: 5.4508 - val_mape: 5.4508\n",
      "Epoch 38/250\n",
      " - 2s - loss: 6.7283 - mape: 6.7283 - val_loss: 7.2567 - val_mape: 7.2567\n",
      "Epoch 39/250\n",
      " - 2s - loss: 6.6476 - mape: 6.6476 - val_loss: 5.4645 - val_mape: 5.4645\n",
      "Epoch 40/250\n",
      " - 2s - loss: 6.8493 - mape: 6.8493 - val_loss: 6.9309 - val_mape: 6.9309\n",
      "Epoch 41/250\n",
      " - 2s - loss: 6.9384 - mape: 6.9384 - val_loss: 6.1550 - val_mape: 6.1550\n",
      "Epoch 42/250\n",
      " - 1s - loss: 6.5277 - mape: 6.5277 - val_loss: 7.7410 - val_mape: 7.7410\n",
      "Epoch 43/250\n",
      " - 1s - loss: 6.4853 - mape: 6.4853 - val_loss: 6.1320 - val_mape: 6.1320\n",
      "Epoch 44/250\n",
      " - 1s - loss: 6.5421 - mape: 6.5421 - val_loss: 8.0270 - val_mape: 8.0270\n",
      "Epoch 45/250\n",
      " - 2s - loss: 6.5267 - mape: 6.5267 - val_loss: 7.4891 - val_mape: 7.4891\n",
      "Epoch 46/250\n",
      " - 2s - loss: 6.4544 - mape: 6.4544 - val_loss: 5.2351 - val_mape: 5.2351\n",
      "Epoch 47/250\n",
      " - 2s - loss: 6.4531 - mape: 6.4531 - val_loss: 6.2643 - val_mape: 6.2643\n",
      "Epoch 48/250\n",
      " - 2s - loss: 6.6372 - mape: 6.6372 - val_loss: 10.1337 - val_mape: 10.1337\n",
      "Epoch 49/250\n",
      " - 2s - loss: 6.4317 - mape: 6.4317 - val_loss: 7.5675 - val_mape: 7.5675\n",
      "Epoch 50/250\n",
      " - 2s - loss: 6.5234 - mape: 6.5234 - val_loss: 6.2803 - val_mape: 6.2803\n",
      "Epoch 51/250\n",
      " - 2s - loss: 6.5095 - mape: 6.5095 - val_loss: 14.6907 - val_mape: 14.6907\n",
      "Epoch 52/250\n",
      " - 2s - loss: 6.6134 - mape: 6.6134 - val_loss: 7.2535 - val_mape: 7.2535\n",
      "Epoch 53/250\n",
      " - 2s - loss: 6.5251 - mape: 6.5251 - val_loss: 7.5433 - val_mape: 7.5433\n",
      "Epoch 54/250\n",
      " - 2s - loss: 6.1369 - mape: 6.1369 - val_loss: 5.6078 - val_mape: 5.6078\n",
      "Epoch 55/250\n",
      " - 2s - loss: 6.2442 - mape: 6.2442 - val_loss: 8.8087 - val_mape: 8.8087\n",
      "Epoch 56/250\n",
      " - 3s - loss: 6.5473 - mape: 6.5473 - val_loss: 5.7970 - val_mape: 5.7970\n",
      "Epoch 57/250\n",
      " - 3s - loss: 6.3124 - mape: 6.3124 - val_loss: 9.7682 - val_mape: 9.7682\n",
      "Epoch 58/250\n",
      " - 3s - loss: 6.1543 - mape: 6.1543 - val_loss: 5.3688 - val_mape: 5.3688\n",
      "Epoch 59/250\n",
      " - 2s - loss: 6.2050 - mape: 6.2050 - val_loss: 5.9122 - val_mape: 5.9122\n",
      "Epoch 60/250\n",
      " - 2s - loss: 6.1395 - mape: 6.1395 - val_loss: 6.2442 - val_mape: 6.2442\n",
      "Epoch 61/250\n",
      " - 2s - loss: 6.3008 - mape: 6.3008 - val_loss: 10.3525 - val_mape: 10.3525\n",
      "Epoch 62/250\n",
      " - 2s - loss: 6.2287 - mape: 6.2287 - val_loss: 5.3591 - val_mape: 5.3591\n",
      "Epoch 63/250\n",
      " - 2s - loss: 6.3357 - mape: 6.3357 - val_loss: 5.0246 - val_mape: 5.0246\n",
      "Epoch 64/250\n",
      " - 2s - loss: 6.1926 - mape: 6.1926 - val_loss: 6.6713 - val_mape: 6.6713\n",
      "Epoch 65/250\n",
      " - 2s - loss: 6.1380 - mape: 6.1380 - val_loss: 4.6577 - val_mape: 4.6577\n",
      "Epoch 66/250\n",
      " - 2s - loss: 6.2113 - mape: 6.2113 - val_loss: 6.0428 - val_mape: 6.0428\n",
      "Epoch 67/250\n",
      " - 2s - loss: 6.0985 - mape: 6.0985 - val_loss: 5.9420 - val_mape: 5.9420\n",
      "Epoch 68/250\n",
      " - 2s - loss: 6.1517 - mape: 6.1517 - val_loss: 8.9144 - val_mape: 8.9144\n",
      "Epoch 69/250\n",
      " - 2s - loss: 6.1327 - mape: 6.1327 - val_loss: 5.5390 - val_mape: 5.5390\n",
      "Epoch 70/250\n",
      " - 2s - loss: 6.1158 - mape: 6.1158 - val_loss: 5.5635 - val_mape: 5.5635\n",
      "Epoch 71/250\n",
      " - 2s - loss: 6.2369 - mape: 6.2369 - val_loss: 5.0860 - val_mape: 5.0860\n",
      "Epoch 72/250\n",
      " - 2s - loss: 5.9123 - mape: 5.9123 - val_loss: 6.1852 - val_mape: 6.1852\n",
      "Epoch 73/250\n",
      " - 2s - loss: 5.8402 - mape: 5.8402 - val_loss: 5.7822 - val_mape: 5.7822\n",
      "Epoch 74/250\n",
      " - 2s - loss: 5.8455 - mape: 5.8455 - val_loss: 4.9919 - val_mape: 4.9918\n",
      "Epoch 75/250\n",
      " - 2s - loss: 5.9251 - mape: 5.9251 - val_loss: 5.2918 - val_mape: 5.2918\n",
      "Epoch 76/250\n",
      " - 2s - loss: 5.9420 - mape: 5.9420 - val_loss: 4.9727 - val_mape: 4.9727\n",
      "Epoch 77/250\n",
      " - 2s - loss: 6.0104 - mape: 6.0104 - val_loss: 8.1868 - val_mape: 8.1868\n",
      "Epoch 78/250\n",
      " - 2s - loss: 6.1411 - mape: 6.1411 - val_loss: 6.6099 - val_mape: 6.6099\n",
      "Epoch 79/250\n",
      " - 2s - loss: 5.7656 - mape: 5.7656 - val_loss: 5.1967 - val_mape: 5.1967\n",
      "Epoch 80/250\n",
      " - 2s - loss: 5.9659 - mape: 5.9659 - val_loss: 7.9558 - val_mape: 7.9558\n",
      "Epoch 81/250\n",
      " - 2s - loss: 5.9814 - mape: 5.9814 - val_loss: 5.8639 - val_mape: 5.8639\n",
      "Epoch 82/250\n",
      " - 2s - loss: 5.7797 - mape: 5.7797 - val_loss: 5.1504 - val_mape: 5.1504\n",
      "Epoch 83/250\n",
      " - 2s - loss: 5.7520 - mape: 5.7520 - val_loss: 4.8243 - val_mape: 4.8243\n",
      "Epoch 84/250\n",
      " - 2s - loss: 5.6117 - mape: 5.6117 - val_loss: 5.0543 - val_mape: 5.0543\n",
      "Epoch 85/250\n",
      " - 2s - loss: 5.8854 - mape: 5.8854 - val_loss: 5.5847 - val_mape: 5.5847\n",
      "Epoch 86/250\n",
      " - 2s - loss: 5.7043 - mape: 5.7043 - val_loss: 5.5305 - val_mape: 5.5305\n",
      "Epoch 87/250\n",
      " - 2s - loss: 6.0207 - mape: 6.0207 - val_loss: 5.3134 - val_mape: 5.3134\n",
      "Epoch 88/250\n",
      " - 1s - loss: 5.5353 - mape: 5.5353 - val_loss: 4.5794 - val_mape: 4.5794\n",
      "Epoch 89/250\n",
      " - 1s - loss: 5.6803 - mape: 5.6803 - val_loss: 6.5077 - val_mape: 6.5077\n",
      "Epoch 90/250\n",
      " - 2s - loss: 5.6084 - mape: 5.6084 - val_loss: 5.7055 - val_mape: 5.7055\n",
      "Epoch 91/250\n",
      " - 2s - loss: 5.6806 - mape: 5.6806 - val_loss: 5.4778 - val_mape: 5.4778\n",
      "Epoch 92/250\n",
      " - 2s - loss: 5.5326 - mape: 5.5326 - val_loss: 5.4741 - val_mape: 5.4741\n",
      "Epoch 93/250\n",
      " - 1s - loss: 5.9063 - mape: 5.9063 - val_loss: 5.7484 - val_mape: 5.7484\n",
      "Epoch 94/250\n",
      " - 1s - loss: 5.5723 - mape: 5.5723 - val_loss: 6.6864 - val_mape: 6.6864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/250\n",
      " - 2s - loss: 5.5472 - mape: 5.5472 - val_loss: 6.4023 - val_mape: 6.4023\n",
      "Epoch 96/250\n",
      " - 1s - loss: 5.6024 - mape: 5.6024 - val_loss: 8.5898 - val_mape: 8.5898\n",
      "Epoch 97/250\n",
      " - 2s - loss: 5.6087 - mape: 5.6087 - val_loss: 6.1055 - val_mape: 6.1055\n",
      "Epoch 98/250\n",
      " - 1s - loss: 5.6307 - mape: 5.6306 - val_loss: 5.3920 - val_mape: 5.3920\n",
      "Epoch 99/250\n",
      " - 1s - loss: 5.4610 - mape: 5.4610 - val_loss: 5.2872 - val_mape: 5.2872\n",
      "Epoch 100/250\n",
      " - 2s - loss: 5.6020 - mape: 5.6020 - val_loss: 4.9734 - val_mape: 4.9734\n",
      "Epoch 101/250\n",
      " - 2s - loss: 5.4414 - mape: 5.4414 - val_loss: 4.4300 - val_mape: 4.4300\n",
      "Epoch 102/250\n",
      " - 2s - loss: 5.6100 - mape: 5.6100 - val_loss: 5.4707 - val_mape: 5.4707\n",
      "Epoch 103/250\n",
      " - 2s - loss: 5.4880 - mape: 5.4880 - val_loss: 4.8663 - val_mape: 4.8663\n",
      "Epoch 104/250\n",
      " - 1s - loss: 5.5098 - mape: 5.5098 - val_loss: 4.8456 - val_mape: 4.8456\n",
      "Epoch 105/250\n",
      " - 2s - loss: 5.5278 - mape: 5.5278 - val_loss: 7.4978 - val_mape: 7.4978\n",
      "Epoch 106/250\n",
      " - 2s - loss: 5.4457 - mape: 5.4457 - val_loss: 5.4147 - val_mape: 5.4147\n",
      "Epoch 107/250\n",
      " - 2s - loss: 5.3871 - mape: 5.3871 - val_loss: 4.2557 - val_mape: 4.2557\n",
      "Epoch 108/250\n",
      " - 1s - loss: 5.3772 - mape: 5.3772 - val_loss: 6.0254 - val_mape: 6.0254\n",
      "Epoch 109/250\n",
      " - 2s - loss: 5.1596 - mape: 5.1596 - val_loss: 5.4720 - val_mape: 5.4720\n",
      "Epoch 110/250\n",
      " - 2s - loss: 5.5497 - mape: 5.5497 - val_loss: 7.0885 - val_mape: 7.0885\n",
      "Epoch 111/250\n",
      " - 2s - loss: 5.9215 - mape: 5.9215 - val_loss: 7.0932 - val_mape: 7.0932\n",
      "Epoch 112/250\n",
      " - 2s - loss: 5.3515 - mape: 5.3515 - val_loss: 5.6433 - val_mape: 5.6433\n",
      "Epoch 113/250\n",
      " - 2s - loss: 5.4165 - mape: 5.4165 - val_loss: 5.8357 - val_mape: 5.8357\n",
      "Epoch 114/250\n",
      " - 2s - loss: 5.4523 - mape: 5.4523 - val_loss: 5.4165 - val_mape: 5.4165\n",
      "Epoch 115/250\n",
      " - 2s - loss: 5.5231 - mape: 5.5231 - val_loss: 5.6339 - val_mape: 5.6339\n",
      "Epoch 116/250\n",
      " - 2s - loss: 5.3552 - mape: 5.3552 - val_loss: 6.0496 - val_mape: 6.0496\n",
      "Epoch 117/250\n",
      " - 2s - loss: 5.3623 - mape: 5.3623 - val_loss: 5.0389 - val_mape: 5.0389\n",
      "Epoch 118/250\n",
      " - 2s - loss: 5.5700 - mape: 5.5700 - val_loss: 4.3610 - val_mape: 4.3610\n",
      "Epoch 119/250\n",
      " - 2s - loss: 5.3681 - mape: 5.3681 - val_loss: 5.2770 - val_mape: 5.2770\n",
      "Epoch 120/250\n",
      " - 3s - loss: 5.2970 - mape: 5.2970 - val_loss: 4.7418 - val_mape: 4.7418\n",
      "Epoch 121/250\n",
      " - 2s - loss: 5.2780 - mape: 5.2780 - val_loss: 5.9710 - val_mape: 5.9710\n",
      "Epoch 122/250\n",
      " - 2s - loss: 5.5270 - mape: 5.5270 - val_loss: 5.3751 - val_mape: 5.3751\n",
      "Epoch 123/250\n",
      " - 2s - loss: 5.4539 - mape: 5.4539 - val_loss: 4.7713 - val_mape: 4.7713\n",
      "Epoch 124/250\n",
      " - 2s - loss: 5.3405 - mape: 5.3405 - val_loss: 4.7925 - val_mape: 4.7925\n",
      "Epoch 125/250\n",
      " - 2s - loss: 5.0704 - mape: 5.0704 - val_loss: 4.0944 - val_mape: 4.0944\n",
      "Epoch 126/250\n",
      " - 1s - loss: 5.3414 - mape: 5.3414 - val_loss: 4.4789 - val_mape: 4.4789\n",
      "Epoch 127/250\n",
      " - 2s - loss: 5.4935 - mape: 5.4935 - val_loss: 6.7069 - val_mape: 6.7069\n",
      "Epoch 128/250\n",
      " - 2s - loss: 5.4876 - mape: 5.4876 - val_loss: 4.7543 - val_mape: 4.7544\n",
      "Epoch 129/250\n",
      " - 2s - loss: 5.2030 - mape: 5.2030 - val_loss: 4.6299 - val_mape: 4.6299\n",
      "Epoch 130/250\n",
      " - 2s - loss: 5.4758 - mape: 5.4758 - val_loss: 4.6467 - val_mape: 4.6467\n",
      "Epoch 131/250\n",
      " - 2s - loss: 5.5416 - mape: 5.5416 - val_loss: 5.3254 - val_mape: 5.3254\n",
      "Epoch 132/250\n",
      " - 2s - loss: 5.4365 - mape: 5.4365 - val_loss: 5.2454 - val_mape: 5.2454\n",
      "Epoch 133/250\n",
      " - 2s - loss: 5.2667 - mape: 5.2667 - val_loss: 4.5753 - val_mape: 4.5753\n",
      "Epoch 134/250\n",
      " - 1s - loss: 5.1299 - mape: 5.1299 - val_loss: 5.1816 - val_mape: 5.1816\n",
      "Epoch 135/250\n",
      " - 1s - loss: 5.5720 - mape: 5.5720 - val_loss: 7.3212 - val_mape: 7.3212\n",
      "Epoch 136/250\n",
      " - 2s - loss: 5.2383 - mape: 5.2383 - val_loss: 3.8705 - val_mape: 3.8705\n",
      "Epoch 137/250\n",
      " - 2s - loss: 5.2067 - mape: 5.2067 - val_loss: 5.1059 - val_mape: 5.1059\n",
      "Epoch 138/250\n",
      " - 2s - loss: 5.3734 - mape: 5.3734 - val_loss: 4.6284 - val_mape: 4.6284\n",
      "Epoch 139/250\n",
      " - 1s - loss: 5.1716 - mape: 5.1716 - val_loss: 5.3706 - val_mape: 5.3706\n",
      "Epoch 140/250\n",
      " - 1s - loss: 5.3272 - mape: 5.3273 - val_loss: 4.2489 - val_mape: 4.2489\n",
      "Epoch 141/250\n",
      " - 1s - loss: 5.1351 - mape: 5.1351 - val_loss: 6.4687 - val_mape: 6.4687\n",
      "Epoch 142/250\n",
      " - 2s - loss: 5.1288 - mape: 5.1288 - val_loss: 4.5811 - val_mape: 4.5811\n",
      "Epoch 143/250\n",
      " - 1s - loss: 5.4809 - mape: 5.4809 - val_loss: 6.1806 - val_mape: 6.1806\n",
      "Epoch 144/250\n",
      " - 1s - loss: 5.1995 - mape: 5.1995 - val_loss: 4.7899 - val_mape: 4.7899\n",
      "Epoch 145/250\n",
      " - 1s - loss: 5.3497 - mape: 5.3497 - val_loss: 5.1601 - val_mape: 5.1601\n",
      "Epoch 146/250\n",
      " - 2s - loss: 5.1566 - mape: 5.1566 - val_loss: 5.5147 - val_mape: 5.5147\n",
      "Epoch 147/250\n",
      " - 2s - loss: 5.0482 - mape: 5.0482 - val_loss: 4.8518 - val_mape: 4.8518\n",
      "Epoch 148/250\n",
      " - 1s - loss: 5.0857 - mape: 5.0857 - val_loss: 5.2972 - val_mape: 5.2972\n",
      "Epoch 149/250\n",
      " - 1s - loss: 5.2101 - mape: 5.2101 - val_loss: 4.4345 - val_mape: 4.4345\n",
      "Epoch 150/250\n",
      " - 1s - loss: 5.1144 - mape: 5.1144 - val_loss: 3.8904 - val_mape: 3.8904\n",
      "Epoch 151/250\n",
      " - 1s - loss: 5.0832 - mape: 5.0832 - val_loss: 4.9257 - val_mape: 4.9257\n",
      "Epoch 152/250\n",
      " - 1s - loss: 5.1279 - mape: 5.1279 - val_loss: 4.9289 - val_mape: 4.9289\n",
      "Epoch 153/250\n",
      " - 1s - loss: 5.0030 - mape: 5.0030 - val_loss: 4.1675 - val_mape: 4.1675\n",
      "Epoch 154/250\n",
      " - 1s - loss: 4.8261 - mape: 4.8261 - val_loss: 4.7483 - val_mape: 4.7483\n",
      "Epoch 155/250\n",
      " - 2s - loss: 5.1401 - mape: 5.1401 - val_loss: 4.6753 - val_mape: 4.6753\n",
      "Epoch 156/250\n",
      " - 2s - loss: 5.0435 - mape: 5.0435 - val_loss: 4.6931 - val_mape: 4.6931\n",
      "Epoch 157/250\n",
      " - 2s - loss: 4.9321 - mape: 4.9321 - val_loss: 3.9318 - val_mape: 3.9318\n",
      "Epoch 158/250\n",
      " - 2s - loss: 5.0736 - mape: 5.0736 - val_loss: 5.1766 - val_mape: 5.1766\n",
      "Epoch 159/250\n",
      " - 1s - loss: 4.7907 - mape: 4.7907 - val_loss: 5.3714 - val_mape: 5.3714\n",
      "Epoch 160/250\n",
      " - 1s - loss: 4.7675 - mape: 4.7675 - val_loss: 4.4128 - val_mape: 4.4128\n",
      "Epoch 161/250\n",
      " - 2s - loss: 4.8538 - mape: 4.8538 - val_loss: 5.7874 - val_mape: 5.7874\n",
      "Epoch 162/250\n",
      " - 2s - loss: 4.8066 - mape: 4.8066 - val_loss: 5.5001 - val_mape: 5.5001\n",
      "Epoch 163/250\n",
      " - 1s - loss: 4.9271 - mape: 4.9271 - val_loss: 4.8349 - val_mape: 4.8349\n",
      "Epoch 164/250\n",
      " - 1s - loss: 4.8683 - mape: 4.8683 - val_loss: 4.1869 - val_mape: 4.1869\n",
      "Epoch 165/250\n",
      " - 1s - loss: 5.0161 - mape: 5.0161 - val_loss: 4.2601 - val_mape: 4.2601\n",
      "Epoch 166/250\n",
      " - 2s - loss: 4.9842 - mape: 4.9842 - val_loss: 4.7366 - val_mape: 4.7366\n",
      "Epoch 167/250\n",
      " - 2s - loss: 4.8933 - mape: 4.8933 - val_loss: 4.2012 - val_mape: 4.2012\n",
      "Epoch 168/250\n",
      " - 2s - loss: 4.8935 - mape: 4.8935 - val_loss: 6.0716 - val_mape: 6.0716\n",
      "Epoch 169/250\n",
      " - 1s - loss: 4.7493 - mape: 4.7493 - val_loss: 4.5169 - val_mape: 4.5169\n",
      "Epoch 170/250\n",
      " - 1s - loss: 4.7711 - mape: 4.7711 - val_loss: 4.7995 - val_mape: 4.7995\n",
      "Epoch 171/250\n",
      " - 2s - loss: 4.8210 - mape: 4.8210 - val_loss: 4.9243 - val_mape: 4.9243\n",
      "Epoch 172/250\n",
      " - 1s - loss: 5.1033 - mape: 5.1033 - val_loss: 4.6590 - val_mape: 4.6590\n",
      "Epoch 173/250\n",
      " - 2s - loss: 4.8153 - mape: 4.8153 - val_loss: 4.7310 - val_mape: 4.7310\n",
      "Epoch 174/250\n",
      " - 2s - loss: 5.0203 - mape: 5.0203 - val_loss: 3.6979 - val_mape: 3.6979\n",
      "Epoch 175/250\n",
      " - 2s - loss: 4.5555 - mape: 4.5555 - val_loss: 4.4917 - val_mape: 4.4917\n",
      "Epoch 176/250\n",
      " - 2s - loss: 4.7088 - mape: 4.7088 - val_loss: 4.3983 - val_mape: 4.3983\n",
      "Epoch 177/250\n",
      " - 2s - loss: 4.6724 - mape: 4.6724 - val_loss: 4.3785 - val_mape: 4.3785\n",
      "Epoch 178/250\n",
      " - 2s - loss: 4.6141 - mape: 4.6141 - val_loss: 3.6909 - val_mape: 3.6909\n",
      "Epoch 179/250\n",
      " - 2s - loss: 4.8684 - mape: 4.8684 - val_loss: 4.3051 - val_mape: 4.3051\n",
      "Epoch 180/250\n",
      " - 1s - loss: 4.9483 - mape: 4.9483 - val_loss: 4.2443 - val_mape: 4.2443\n",
      "Epoch 181/250\n",
      " - 1s - loss: 4.5426 - mape: 4.5426 - val_loss: 4.3185 - val_mape: 4.3185\n",
      "Epoch 182/250\n",
      " - 1s - loss: 4.6767 - mape: 4.6767 - val_loss: 5.0309 - val_mape: 5.0309\n",
      "Epoch 183/250\n",
      " - 1s - loss: 4.8537 - mape: 4.8537 - val_loss: 4.6323 - val_mape: 4.6323\n",
      "Epoch 184/250\n",
      " - 1s - loss: 4.7388 - mape: 4.7388 - val_loss: 4.2996 - val_mape: 4.2996\n",
      "Epoch 185/250\n",
      " - 1s - loss: 4.7811 - mape: 4.7811 - val_loss: 5.7388 - val_mape: 5.7388\n",
      "Epoch 186/250\n",
      " - 2s - loss: 4.7686 - mape: 4.7686 - val_loss: 4.5808 - val_mape: 4.5808\n",
      "Epoch 187/250\n",
      " - 2s - loss: 4.7787 - mape: 4.7787 - val_loss: 6.6728 - val_mape: 6.6728\n",
      "Epoch 188/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 4.9679 - mape: 4.9679 - val_loss: 4.5103 - val_mape: 4.5103\n",
      "Epoch 189/250\n",
      " - 2s - loss: 4.6290 - mape: 4.6290 - val_loss: 5.2227 - val_mape: 5.2227\n",
      "Epoch 190/250\n",
      " - 2s - loss: 4.8212 - mape: 4.8212 - val_loss: 4.9717 - val_mape: 4.9717\n",
      "Epoch 191/250\n",
      " - 2s - loss: 4.7470 - mape: 4.7470 - val_loss: 3.5515 - val_mape: 3.5515\n",
      "Epoch 192/250\n",
      " - 2s - loss: 4.6959 - mape: 4.6959 - val_loss: 4.7777 - val_mape: 4.7777\n",
      "Epoch 193/250\n",
      " - 2s - loss: 4.7325 - mape: 4.7325 - val_loss: 4.7881 - val_mape: 4.7881\n",
      "Epoch 194/250\n",
      " - 2s - loss: 4.7582 - mape: 4.7582 - val_loss: 4.4244 - val_mape: 4.4244\n",
      "Epoch 195/250\n",
      " - 2s - loss: 4.7459 - mape: 4.7459 - val_loss: 3.5433 - val_mape: 3.5433\n",
      "Epoch 196/250\n",
      " - 1s - loss: 4.6891 - mape: 4.6891 - val_loss: 6.3820 - val_mape: 6.3820\n",
      "Epoch 197/250\n",
      " - 2s - loss: 4.6426 - mape: 4.6426 - val_loss: 5.2537 - val_mape: 5.2537\n",
      "Epoch 198/250\n",
      " - 2s - loss: 4.8198 - mape: 4.8198 - val_loss: 4.1521 - val_mape: 4.1521\n",
      "Epoch 199/250\n",
      " - 2s - loss: 4.6839 - mape: 4.6839 - val_loss: 4.0774 - val_mape: 4.0774\n",
      "Epoch 200/250\n",
      " - 1s - loss: 4.5902 - mape: 4.5902 - val_loss: 3.6714 - val_mape: 3.6714\n",
      "Epoch 201/250\n",
      " - 1s - loss: 4.5274 - mape: 4.5274 - val_loss: 4.2353 - val_mape: 4.2353\n",
      "Epoch 202/250\n",
      " - 1s - loss: 4.6085 - mape: 4.6085 - val_loss: 5.6269 - val_mape: 5.6269\n",
      "Epoch 203/250\n",
      " - 2s - loss: 4.7228 - mape: 4.7228 - val_loss: 4.1661 - val_mape: 4.1661\n",
      "Epoch 204/250\n",
      " - 2s - loss: 4.6435 - mape: 4.6435 - val_loss: 5.7821 - val_mape: 5.7821\n",
      "Epoch 205/250\n",
      " - 2s - loss: 4.6513 - mape: 4.6512 - val_loss: 4.4514 - val_mape: 4.4514\n",
      "Epoch 206/250\n",
      " - 2s - loss: 4.6950 - mape: 4.6950 - val_loss: 4.5134 - val_mape: 4.5134\n",
      "Epoch 207/250\n",
      " - 2s - loss: 4.7016 - mape: 4.7016 - val_loss: 3.9463 - val_mape: 3.9463\n",
      "Epoch 208/250\n",
      " - 2s - loss: 4.6761 - mape: 4.6761 - val_loss: 4.0498 - val_mape: 4.0498\n",
      "Epoch 209/250\n",
      " - 2s - loss: 4.8088 - mape: 4.8088 - val_loss: 4.0428 - val_mape: 4.0428\n",
      "Epoch 210/250\n",
      " - 2s - loss: 4.6754 - mape: 4.6754 - val_loss: 6.4207 - val_mape: 6.4207\n",
      "Epoch 211/250\n",
      " - 2s - loss: 4.6849 - mape: 4.6849 - val_loss: 3.8300 - val_mape: 3.8300\n",
      "Epoch 212/250\n",
      " - 2s - loss: 4.7047 - mape: 4.7047 - val_loss: 5.4619 - val_mape: 5.4619\n",
      "Epoch 213/250\n",
      " - 2s - loss: 4.7855 - mape: 4.7855 - val_loss: 5.0962 - val_mape: 5.0962\n",
      "Epoch 214/250\n",
      " - 1s - loss: 4.7227 - mape: 4.7227 - val_loss: 4.1697 - val_mape: 4.1697\n",
      "Epoch 215/250\n",
      " - 2s - loss: 4.5910 - mape: 4.5910 - val_loss: 3.7832 - val_mape: 3.7832\n",
      "Epoch 216/250\n",
      " - 2s - loss: 4.7351 - mape: 4.7351 - val_loss: 4.1456 - val_mape: 4.1456\n",
      "Epoch 217/250\n",
      " - 1s - loss: 4.5535 - mape: 4.5535 - val_loss: 4.0024 - val_mape: 4.0024\n",
      "Epoch 218/250\n",
      " - 2s - loss: 4.6761 - mape: 4.6761 - val_loss: 4.9642 - val_mape: 4.9642\n",
      "Epoch 219/250\n",
      " - 1s - loss: 4.4701 - mape: 4.4701 - val_loss: 4.6685 - val_mape: 4.6685\n",
      "Epoch 220/250\n",
      " - 1s - loss: 4.5978 - mape: 4.5978 - val_loss: 4.7445 - val_mape: 4.7445\n",
      "Epoch 221/250\n",
      " - 2s - loss: 4.7271 - mape: 4.7271 - val_loss: 4.0491 - val_mape: 4.0491\n",
      "Epoch 222/250\n",
      " - 1s - loss: 4.5415 - mape: 4.5415 - val_loss: 5.5072 - val_mape: 5.5072\n",
      "Epoch 223/250\n",
      " - 2s - loss: 4.6608 - mape: 4.6608 - val_loss: 4.9491 - val_mape: 4.9491\n",
      "Epoch 224/250\n",
      " - 2s - loss: 4.6315 - mape: 4.6315 - val_loss: 3.8334 - val_mape: 3.8334\n",
      "Epoch 225/250\n",
      " - 2s - loss: 4.6055 - mape: 4.6055 - val_loss: 5.3338 - val_mape: 5.3338\n",
      "Epoch 226/250\n",
      " - 2s - loss: 4.6845 - mape: 4.6845 - val_loss: 4.2894 - val_mape: 4.2894\n",
      "Epoch 227/250\n",
      " - 1s - loss: 4.4595 - mape: 4.4595 - val_loss: 3.5248 - val_mape: 3.5248\n",
      "Epoch 228/250\n",
      " - 1s - loss: 4.6009 - mape: 4.6009 - val_loss: 3.5802 - val_mape: 3.5802\n",
      "Epoch 229/250\n",
      " - 2s - loss: 4.5987 - mape: 4.5987 - val_loss: 5.4763 - val_mape: 5.4763\n",
      "Epoch 230/250\n",
      " - 2s - loss: 4.5251 - mape: 4.5251 - val_loss: 4.8477 - val_mape: 4.8477\n",
      "Epoch 231/250\n",
      " - 2s - loss: 4.6607 - mape: 4.6607 - val_loss: 3.8946 - val_mape: 3.8946\n",
      "Epoch 232/250\n",
      " - 1s - loss: 4.6515 - mape: 4.6515 - val_loss: 5.1042 - val_mape: 5.1042\n",
      "Epoch 233/250\n",
      " - 2s - loss: 4.7452 - mape: 4.7452 - val_loss: 3.6871 - val_mape: 3.6871\n",
      "Epoch 234/250\n",
      " - 2s - loss: 4.4517 - mape: 4.4517 - val_loss: 3.5797 - val_mape: 3.5797\n",
      "Epoch 235/250\n",
      " - 2s - loss: 4.6570 - mape: 4.6570 - val_loss: 3.7435 - val_mape: 3.7435\n",
      "Epoch 236/250\n",
      " - 2s - loss: 4.6567 - mape: 4.6567 - val_loss: 3.4039 - val_mape: 3.4039\n",
      "Epoch 237/250\n",
      " - 2s - loss: 4.5753 - mape: 4.5753 - val_loss: 5.3582 - val_mape: 5.3582\n",
      "Epoch 238/250\n",
      " - 1s - loss: 4.6373 - mape: 4.6373 - val_loss: 3.4411 - val_mape: 3.4411\n",
      "Epoch 239/250\n",
      " - 2s - loss: 4.4836 - mape: 4.4836 - val_loss: 3.4756 - val_mape: 3.4756\n",
      "Epoch 240/250\n",
      " - 1s - loss: 4.7000 - mape: 4.7000 - val_loss: 4.3211 - val_mape: 4.3211\n",
      "Epoch 241/250\n",
      " - 1s - loss: 4.5430 - mape: 4.5430 - val_loss: 3.7049 - val_mape: 3.7049\n",
      "Epoch 242/250\n",
      " - 1s - loss: 4.5339 - mape: 4.5339 - val_loss: 4.1108 - val_mape: 4.1108\n",
      "Epoch 243/250\n",
      " - 1s - loss: 4.4389 - mape: 4.4389 - val_loss: 4.0753 - val_mape: 4.0753\n",
      "Epoch 244/250\n",
      " - 2s - loss: 4.6492 - mape: 4.6492 - val_loss: 4.5661 - val_mape: 4.5661\n",
      "Epoch 245/250\n",
      " - 2s - loss: 4.5025 - mape: 4.5025 - val_loss: 4.0196 - val_mape: 4.0196\n",
      "Epoch 246/250\n",
      " - 2s - loss: 4.5826 - mape: 4.5826 - val_loss: 5.0596 - val_mape: 5.0596\n",
      "Epoch 247/250\n",
      " - 1s - loss: 4.7556 - mape: 4.7556 - val_loss: 4.2298 - val_mape: 4.2298\n",
      "Epoch 248/250\n",
      " - 1s - loss: 4.6383 - mape: 4.6383 - val_loss: 4.1462 - val_mape: 4.1462\n",
      "Epoch 249/250\n",
      " - 2s - loss: 4.4264 - mape: 4.4264 - val_loss: 3.8777 - val_mape: 3.8777\n",
      "Epoch 250/250\n",
      " - 2s - loss: 4.6316 - mape: 4.6316 - val_loss: 4.5047 - val_mape: 4.5047\n"
     ]
    }
   ],
   "source": [
    "evolution = model.fit(X_train, y_train,\n",
    "         batch_size=10,\n",
    "         epochs=250,\n",
    "         verbose=2,\n",
    "         validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(evolution.history['mape'])\n",
    "plt.plot(evolution.history['val_mape'])\n",
    "plt.title(\"Model MAPE\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 107us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.504660129547119"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sans optimisation des paramètres, notre réseau de neurones prédit les 3 bonnes valeurs de sortie avec une erreur MAPE de 4.5 environ**\n",
    "\n",
    "# Hyper Paramétrisation\n",
    "\n",
    "Après avoir déterminé la profondeur optimale du réseau, nous souhaitons maintenant trouver les paramètres optimaux du modèle. Pour cela, nous utilisons la fonction GridSearchCV. \n",
    "Cette fonction n'admet pas MAPE comme score pour retenir le meilleur modèle. Nous avons donc essayé d'implémenter nous même la fonction \"mean_absolute_percentage_error\" dans la fonction GridSearchCV par l'intermédiaire de la variable my_scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "my_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons notre modèle dans une fonction \"create_model\" en faisant varier certains paramètres tels que :\n",
    "- l'initialisation des poids \"init_mode\".\n",
    "- l’algorithme d'optimisation pour la descente du gradient \"optimizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(init_mode='uniform', optimizer='adam'):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=8, \n",
    "                kernel_initializer=init_mode, \n",
    "                activation='relu')) \n",
    "\n",
    "    model.add(Dense(20, kernel_initializer=init_mode, \n",
    "                activation='relu'))\n",
    "    \n",
    "    model.add(Dense(3, kernel_initializer=init_mode, activation='linear'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='mean_absolute_percentage_error', \n",
    "              optimizer=optimizer, \n",
    "              metrics = ['mape']) #une ou plrs fonction pour évaluer le modèle\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de pourvoir utiliser notre modèle keras dans la fonction GridSearchCV de sklearn, nous devons utiliser la fonction KerasRegressor (ou KerasClassifier pour de la classification). Cette fonction nous permet de mettre les paramètres d'entrainement du réseau afin qu'il soit utilisé dans GridSearchCV.\n",
    "\n",
    "Enfin, nous avons sélectionné sélectionner la liste des paramètres que nous souhaitons tester dans la variable \"param_grid\". Dans ce code avons laissé seulement 2 possibilités par variables pour réduire le temps de calcul. Mais dans la réalité, nous avons testé avec tous les paramètres possibles, et nous avons laissé les paramètres ayant eu les meilleurs résultats : c'est à dire :\n",
    "\n",
    "init_mode = ['uniform', 'normal']  et   optimizer = ['Adam', 'Adamax']\n",
    "\n",
    "La fonction gridSearchCV test toutes les combinaisons possible des paramètres en réalisant 3 cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed: 23.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 3s 377us/step - loss: 46.1182 - mape: 46.1182\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 29.9570 - mape: 29.9570\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 21.7245 - mape: 21.7245\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 19.9643 - mape: 19.9643\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 17.2448 - mape: 17.2448\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 13.2764 - mape: 13.2764\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 11.6744 - mape: 11.6744\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 10.6433 - mape: 10.6433\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 10.2747 - mape: 10.2747\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 9.9249 - mape: 9.9249\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 2s 306us/step - loss: 9.7678 - mape: 9.7678\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 3s 336us/step - loss: 9.5752 - mape: 9.5752\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 9.3258 - mape: 9.3258\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 8.7785 - mape: 8.7785\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 8.2589 - mape: 8.2589\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 7.9087 - mape: 7.9087\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: 7.8667 - mape: 7.8667\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 7.7337 - mape: 7.7337\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 7.9077 - mape: 7.9077\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 7.9499 - mape: 7.9499\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 7.6857 - mape: 7.6857\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 7.3796 - mape: 7.3796\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 7.3222 - mape: 7.3222\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 7.0629 - mape: 7.0629\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 6.9564 - mape: 6.9564\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 6.7921 - mape: 6.7921\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 6.2453 - mape: 6.2453\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 6.4444 - mape: 6.4444\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 6.6632 - mape: 6.6632\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 6.6680 - mape: 6.6680\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 6.3115 - mape: 6.3115\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 6.6109 - mape: 6.6109\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 6.3327 - mape: 6.3327\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 6.4499 - mape: 6.4499\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 6.4659 - mape: 6.4659\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 6.0075 - mape: 6.0075\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 6.1968 - mape: 6.1968\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 5.9671 - mape: 5.9671\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: 6.2547 - mape: 6.2547\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 6.0413 - mape: 6.0413\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 6.0862 - mape: 6.0862\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 6.1444 - mape: 6.1444\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 6.1868 - mape: 6.1868\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 5.7911 - mape: 5.7911\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 5.9856 - mape: 5.9856\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 5.7671 - mape: 5.7671\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 5.7518 - mape: 5.7518\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 5.9156 - mape: 5.9156\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 5.4789 - mape: 5.4789\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 5.5093 - mape: 5.50 - 2s 210us/step - loss: 5.4925 - mape: 5.4925\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 5.8707 - mape: 5.8707\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 5.4071 - mape: 5.4071\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 5.7982 - mape: 5.7982\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 5.5893 - mape: 5.5893\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 5.6199 - mape: 5.6199\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 5.4912 - mape: 5.4912\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 5.6462 - mape: 5.6462\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 5.0986 - mape: 5.0986\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 5.5004 - mape: 5.5004\n",
      "Epoch 60/200\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 5.3947 - mape: 5.3947\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 5.3459 - mape: 5.3459\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 4.9772 - mape: 4.9772\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 5.2268 - mape: 5.2268\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 5.2763 - mape: 5.2763\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 5.4694 - mape: 5.4694\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 5.7446 - mape: 5.7446\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 5.2443 - mape: 5.2443\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 5.0463 - mape: 5.0463\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 5.1152 - mape: 5.1152\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 5.4089 - mape: 5.4089\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 5.1274 - mape: 5.1274\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 5.0107 - mape: 5.0107\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 2s 294us/step - loss: 5.0126 - mape: 5.0126\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 4.9883 - mape: 4.9883\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 4.9206 - mape: 4.9206\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 4.9638 - mape: 4.9638\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 4.8629 - mape: 4.8629\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 4.9701 - mape: 4.9701\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 4.9210 - mape: 4.9210\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 5.0080 - mape: 5.0080\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 4.9159 - mape: 4.9159\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 5.0682 - mape: 5.0682\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 4.9479 - mape: 4.9479\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 4.8232 - mape: 4.8232\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 4.7878 - mape: 4.7878\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 5.0041 - mape: 5.0041\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 5.0019 - mape: 5.0019\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 4.9596 - mape: 4.9596\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 4.6465 - mape: 4.6465\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 4.5553 - mape: 4.5553\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 2s 272us/step - loss: 4.6518 - mape: 4.6518\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 4.6364 - mape: 4.6364\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 4.8762 - mape: 4.8762\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 4.7085 - mape: 4.7085\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 4.5759 - mape: 4.5759\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 4.6482 - mape: 4.6482\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 4.5625 - mape: 4.5625\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 4.6886 - mape: 4.6886\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 4.8202 - mape: 4.8202\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 4.4446 - mape: 4.4446\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 4.6863 - mape: 4.6863\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 4.5404 - mape: 4.5404\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 4.7199 - mape: 4.7199\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 4.4077 - mape: 4.4077\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 4.4270 - mape: 4.4270\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 4.3385 - mape: 4.3385\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 4.5509 - mape: 4.5509\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 4.6996 - mape: 4.6996\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 4.5933 - mape: 4.5933\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 4.4990 - mape: 4.4990\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 4.5280 - mape: 4.5280\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 4.5555 - mape: 4.5555\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 4.6487 - mape: 4.6487\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 4.4177 - mape: 4.4177\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 4.5938 - mape: 4.5938\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 4.4511 - mape: 4.4511\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 4.3952 - mape: 4.3952\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 4.4570 - mape: 4.4570\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 4.5392 - mape: 4.5392\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 4.4019 - mape: 4.4019\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 4.5493 - mape: 4.5493\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 4.5213 - mape: 4.5213\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 4.4677 - mape: 4.4677\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 4.3319 - mape: 4.3319\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 4.5110 - mape: 4.5110\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 4.5195 - mape: 4.5195\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 4.4964 - mape: 4.4964\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 4.6123 - mape: 4.6123\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 4.4301 - mape: 4.4301\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 4.3685 - mape: 4.3685\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 4.2061 - mape: 4.2061\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 4.3702 - mape: 4.3702\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 4.1511 - mape: 4.1511\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 4.3897 - mape: 4.3897\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 4.5646 - mape: 4.5646\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 4.4793 - mape: 4.4793\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 4.2954 - mape: 4.2954\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 4.4671 - mape: 4.4671\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 4.1837 - mape: 4.1837\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 4.3765 - mape: 4.3765\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 4.2298 - mape: 4.2298\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 4.3821 - mape: 4.3821\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 4.3554 - mape: 4.3554\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 4.0593 - mape: 4.0593\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 4.0784 - mape: 4.0784\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 4.2562 - mape: 4.2562\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 4.2838 - mape: 4.2839\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 4.0837 - mape: 4.0837\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 4.3951 - mape: 4.3951\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 4.2021 - mape: 4.2021\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 4.3753 - mape: 4.3753\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 4.4438 - mape: 4.4438\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 4.3071 - mape: 4.3071\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 4.0909 - mape: 4.0909\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 4.3669 - mape: 4.3669\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 4.0165 - mape: 4.0165\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 4.1658 - mape: 4.1658\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 4.1219 - mape: 4.1219\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 219us/step - loss: 4.2417 - mape: 4.2417\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 4.0332 - mape: 4.0332\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 4.2046 - mape: 4.2046\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 4.1210 - mape: 4.1210\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 4.2348 - mape: 4.2348\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 4.2540 - mape: 4.2540\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 4.3252 - mape: 4.3252\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 4.3262 - mape: 4.3262\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 4.2611 - mape: 4.2611\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 4.2408 - mape: 4.2408\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 4.0963 - mape: 4.0963\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 4.2677 - mape: 4.2677\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 4.1026 - mape: 4.1026\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 4.2543 - mape: 4.2543\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 3.9937 - mape: 3.9937\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 4.1735 - mape: 4.1735\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 4.2113 - mape: 4.2113\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 4.1165 - mape: 4.1165\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 4.1098 - mape: 4.1098\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 4.1145 - mape: 4.1145\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 4.2707 - mape: 4.2707\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 4.1781 - mape: 4.1781\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 4.1233 - mape: 4.1233\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 4.0734 - mape: 4.0734\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 4.1595 - mape: 4.1595\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 4.0083 - mape: 4.0083\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 4.1829 - mape: 4.1829\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 4.2052 - mape: 4.2052\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 4.3349 - mape: 4.3349\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 4.0854 - mape: 4.0854\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 3.9491 - mape: 3.9491\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 4.1519 - mape: 4.1519\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 4.1432 - mape: 4.1432\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 4.0994 - mape: 4.0994\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 2s 291us/step - loss: 4.1120 - mape: 4.1120\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 2s 291us/step - loss: 4.0638 - mape: 4.0638\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 3.9672 - mape: 3.9672\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 4.2140 - mape: 4.2140\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 4.0244 - mape: 4.0244\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 4.1030 - mape: 4.1030\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 4.3854 - mape: 4.3854\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 4.1209 - mape: 4.1209\n"
     ]
    }
   ],
   "source": [
    "model_CV = KerasRegressor(build_fn=create_model, epochs=200, \n",
    "                           batch_size=10, verbose=1)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'normal']\n",
    "optimizer = ['Adam', 'Adamax']\n",
    "param_grid = dict(init_mode=init_mode, optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3, scoring=my_scorer, verbose=1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mape for 4.6358727711382155 using {'init_mode': 'normal', 'optimizer': 'Adamax'}\n",
      " mean=4.796, std=0.6638 using {'init_mode': 'uniform', 'optimizer': 'Adam'}\n",
      " mean=4.72, std=1.517 using {'init_mode': 'uniform', 'optimizer': 'Adamax'}\n",
      " mean=5.54, std=1.46 using {'init_mode': 'normal', 'optimizer': 'Adam'}\n",
      " mean=4.636, std=0.6293 using {'init_mode': 'normal', 'optimizer': 'Adamax'}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Mape for {np.abs(grid_result.best_score_)} using {grid_result.best_params_}')\n",
    "means = np.abs(grid_result.cv_results_['mean_test_score'])\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les meilleurs résultats de MAPE sont donc obtenus lorsque nous avons une initialisation \"normal\" et un optimizer \"Adamax\" même si nous remarquons que tous les résultats sont très similaires / proches.\n",
    "\n",
    "Nous allons vérifier le nombre de neurones opital lorsque l'initialisation est \"normal\" et l'optimizer est \"Adamax\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons1 = 20, neurons2=20):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=8, \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu')) \n",
    "\n",
    "    model.add(Dense(neurons2, kernel_initializer='normal', \n",
    "                activation='relu'))\n",
    "    \n",
    "    model.add(Dense(3, kernel_initializer='normal', activation='linear'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='mean_absolute_percentage_error', \n",
    "              optimizer='Adamax', \n",
    "              metrics = ['mape']) #une ou plrs fonction pour évaluer le modèle\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 121 candidates, totalling 363 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 10.3min\n",
      "C:\\Users\\CORTIAL\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 43.4min\n",
      "[Parallel(n_jobs=-1)]: Done 363 out of 363 | elapsed: 80.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 51.6353 - mape: 51.6353\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 45.7619 - mape: 45.7619\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 44.7819 - mape: 44.7819\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 43.5833 - mape: 43.5833\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 42.1041 - mape: 42.1041\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 40.2075 - mape: 40.2075\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 37.7857 - mape: 37.7857\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 34.6818 - mape: 34.6818\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 30.9929 - mape: 30.9929\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 27.3268 - mape: 27.3268\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 24.8642 - mape: 24.8642\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 23.3529 - mape: 23.3529\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 22.4693 - mape: 22.4693\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 21.9300 - mape: 21.9300\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 21.4912 - mape: 21.4912\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 20.9168 - mape: 20.9168\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 20.6930 - mape: 20.6930\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 20.6361 - mape: 20.6361\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 19.8600 - mape: 19.8600\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 19.5031 - mape: 19.5031\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 18.9181 - mape: 18.9181\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 18.6059 - mape: 18.6059\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 17.7836 - mape: 17.7837\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 17.4541 - mape: 17.4541\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 16.5223 - mape: 16.5223\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 15.2107 - mape: 15.2107\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 14.2013 - mape: 14.2013\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 13.6928 - mape: 13.6928\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 13.0240 - mape: 13.0240\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 12.0524 - mape: 12.0524\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 11.0286 - mape: 11.0286\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 10.5597 - mape: 10.5597\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 10.5734 - mape: 10.5734\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 10.4109 - mape: 10.4109\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 10.4800 - mape: 10.4800\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 10.4293 - mape: 10.4293\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 9.8287 - mape: 9.8287\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 9.8025 - mape: 9.8025\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 10.1330 - mape: 10.1330 0s - loss: 9.9833 - mape: 9.\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 0s 37us/step - loss: 9.9300 - mape: 9.9300\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 9.7934 - mape: 9.7934\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 9.4285 - mape: 9.4285\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 9.4478 - mape: 9.4478\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 9.2077 - mape: 9.2077\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 9.1604 - mape: 9.1604\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 9.5434 - mape: 9.5434A: 0s - loss: 9.6184 - mape\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 8.9063 - mape: 8.9063\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 8.8134 - mape: 8.8134\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 8.8885 - mape: 8.8885\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 8.8239 - mape: 8.8239\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 9.0826 - mape: 9.0826\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 8.6196 - mape: 8.6196\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 8.9534 - mape: 8.9534\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 8.8830 - mape: 8.8830\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 8.2995 - mape: 8.2995\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 8.2706 - mape: 8.2706\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 7.8849 - mape: 7.8849\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 7.8991 - mape: 7.8990\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 7.9374 - mape: 7.9374\n",
      "Epoch 60/200\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 7.8549 - mape: 7.8549\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 7.9734 - mape: 7.9734\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 0s 35us/step - loss: 7.6228 - mape: 7.6228\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 7.7943 - mape: 7.7943 0s - loss: 7.2805 - mape\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 7.2993 - mape: 7.2993\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 7.5800 - mape: 7.5800\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 7.8480 - mape: 7.8480\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 7.3263 - mape: 7.3263\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 7.2234 - mape: 7.2234\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 7.4928 - mape: 7.4928\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 7.3347 - mape: 7.3347\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 7.1081 - mape: 7.1081\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 7.0326 - mape: 7.0326\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 7.0062 - mape: 7.0062\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 7.1588 - mape: 7.1588\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 7.0683 - mape: 7.0683\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 7.1178 - mape: 7.1178\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 7.0597 - mape: 7.0597\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 6.8655 - mape: 6.8655\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 6.7236 - mape: 6.7236\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 6.9284 - mape: 6.9284\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 6.8312 - mape: 6.8312\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 6.6781 - mape: 6.6781\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 6.5610 - mape: 6.5610\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 6.5190 - mape: 6.5190\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 6.5425 - mape: 6.5425\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 6.8251 - mape: 6.8251\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 6.8010 - mape: 6.8010\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 6.6610 - mape: 6.6610\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 6.3609 - mape: 6.3609\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 6.4469 - mape: 6.4469\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 6.8012 - mape: 6.8012\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 6.5561 - mape: 6.5561\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 6.5819 - mape: 6.5819\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 6.2263 - mape: 6.2263\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 6.5878 - mape: 6.5878\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 5.9151 - mape: 5.9151\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 6.5674 - mape: 6.56 - 0s 30us/step - loss: 6.5576 - mape: 6.5576\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 6.5566 - mape: 6.5566\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 6.4956 - mape: 6.4956\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 5.9134 - mape: 5.9134\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 6.1212 - mape: 6.1212 0s - loss: 6.6673 - mape\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 6.1843 - mape: 6.1843\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 6.1187 - mape: 6.1187\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 6.4330 - mape: 6.4330\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 6.4857 - mape: 6.4857\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 6.2254 - mape: 6.2254\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 5.8400 - mape: 5.8400\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 6.1841 - mape: 6.1841\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 6.1562 - mape: 6.1562\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 5.9846 - mape: 5.9846\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 6.0402 - mape: 6.0402\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.8893 - mape: 5.8893\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 6.0466 - mape: 6.0466\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 5.9171 - mape: 5.9171\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 5.8525 - mape: 5.8525\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 5.8280 - mape: 5.8280\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 6.0388 - mape: 6.0388\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 6.1161 - mape: 6.1161\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 5.8566 - mape: 5.8566\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 5.9960 - mape: 5.9960\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.8591 - mape: 5.8591\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.8358 - mape: 5.8358\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.8364 - mape: 5.8364\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.5689 - mape: 5.5689\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 6.0547 - mape: 6.0547\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 5.4906 - mape: 5.4906\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.6786 - mape: 5.6786\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.7555 - mape: 5.7555\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 5.4799 - mape: 5.4799\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 5.2747 - mape: 5.2747\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.5926 - mape: 5.5926\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.6495 - mape: 5.6495\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.6077 - mape: 5.6077\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 5.3856 - mape: 5.3856\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 5.5842 - mape: 5.5842\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 5.6360 - mape: 5.6360\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.9207 - mape: 5.9207\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.3630 - mape: 5.3630\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 5.1924 - mape: 5.1924\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 5.6078 - mape: 5.6078\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.6047 - mape: 5.6047\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.5367 - mape: 5.5367\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.3350 - mape: 5.3350\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 5.0567 - mape: 5.0567\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.9374 - mape: 5.9374\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 5.5965 - mape: 5.5965\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.3163 - mape: 5.3163\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.2974 - mape: 5.2974\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.3185 - mape: 5.3185\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.8373 - mape: 5.8373\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.0409 - mape: 5.0409\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 4.9772 - mape: 4.9772\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 5.6594 - mape: 5.6594\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 5.3322 - mape: 5.3322\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 5.0511 - mape: 5.0511\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.0069 - mape: 5.0069\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 4.9319 - mape: 4.9319\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.4044 - mape: 5.4044\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 4.9315 - mape: 4.9315\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.4115 - mape: 5.4115\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 4.8773 - mape: 4.8773\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 4.8432 - mape: 4.8432\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.3704 - mape: 5.3704\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 4.9040 - mape: 4.9040\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.1132 - mape: 5.1132\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 4.9523 - mape: 4.9523\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 4.9848 - mape: 4.9848 0s - loss: 4.9590 - mape: \n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 5.1037 - mape: 5.1037\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 4.9738 - mape: 4.9738\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 4.6212 - mape: 4.6212\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 4.9035 - mape: 4.9035\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 5.4840 - mape: 5.4840\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 5.0580 - mape: 5.0580\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 5.1396 - mape: 5.1396\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 4.9116 - mape: 4.9116\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 4.6528 - mape: 4.6528\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 4.9795 - mape: 4.9795\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 5.1735 - mape: 5.1735\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 5.1330 - mape: 5.1330\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 4.9540 - mape: 4.9540\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 0s 32us/step - loss: 4.8687 - mape: 4.8687\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 5.0096 - mape: 5.0096\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 4.8194 - mape: 4.8194\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 5.4275 - mape: 5.4275\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 4.7477 - mape: 4.7477\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 6.7367 - mape: 6.7367\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 5.3992 - mape: 5.3992\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 4.8025 - mape: 4.8025\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 4.8480 - mape: 4.8480\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 4.9743 - mape: 4.9743\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 4.9321 - mape: 4.9321\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 4.5821 - mape: 4.5821\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.0742 - mape: 5.0742\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 4.9119 - mape: 4.9119\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 4.9690 - mape: 4.9690\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 4.7538 - mape: 4.7538\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 5.1947 - mape: 5.1947\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 4.6976 - mape: 4.6976\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 5.1969 - mape: 5.1968\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 4.3934 - mape: 4.3934\n"
     ]
    }
   ],
   "source": [
    "model_CV = KerasRegressor(build_fn=create_model, epochs=200, \n",
    "                           batch_size=100, verbose=1)\n",
    "# define the grid search parameters\n",
    "neurons1 = [20,21,22,23,24,25,26,27,28,29,30]\n",
    "neurons2 = [20,21,22,23,24,25,26,27,28,29,30]\n",
    "param_grid = dict(neurons1 = neurons1, neurons2 = neurons2 )\n",
    "grid = GridSearchCV(estimator=model_CV, param_grid=param_grid, n_jobs=-1, cv=3, scoring=my_scorer, verbose=1)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mape for 4.679410720843589 using {'neurons1': 28, 'neurons2': 25}\n",
      " mean=6.62, std=0.9664 using {'neurons1': 20, 'neurons2': 20}\n",
      " mean=7.365, std=0.3846 using {'neurons1': 20, 'neurons2': 21}\n",
      " mean=5.963, std=0.9705 using {'neurons1': 20, 'neurons2': 22}\n",
      " mean=6.395, std=1.3 using {'neurons1': 20, 'neurons2': 23}\n",
      " mean=5.327, std=0.8917 using {'neurons1': 20, 'neurons2': 24}\n",
      " mean=5.699, std=0.906 using {'neurons1': 20, 'neurons2': 25}\n",
      " mean=6.384, std=1.416 using {'neurons1': 20, 'neurons2': 26}\n",
      " mean=6.336, std=0.5294 using {'neurons1': 20, 'neurons2': 27}\n",
      " mean=5.531, std=0.7719 using {'neurons1': 20, 'neurons2': 28}\n",
      " mean=8.166, std=0.7415 using {'neurons1': 20, 'neurons2': 29}\n",
      " mean=5.448, std=0.4553 using {'neurons1': 20, 'neurons2': 30}\n",
      " mean=6.081, std=0.3213 using {'neurons1': 21, 'neurons2': 20}\n",
      " mean=6.369, std=1.079 using {'neurons1': 21, 'neurons2': 21}\n",
      " mean=5.347, std=0.6403 using {'neurons1': 21, 'neurons2': 22}\n",
      " mean=5.94, std=0.2089 using {'neurons1': 21, 'neurons2': 23}\n",
      " mean=6.104, std=0.9598 using {'neurons1': 21, 'neurons2': 24}\n",
      " mean=5.044, std=0.4595 using {'neurons1': 21, 'neurons2': 25}\n",
      " mean=5.717, std=0.6327 using {'neurons1': 21, 'neurons2': 26}\n",
      " mean=6.85, std=0.3334 using {'neurons1': 21, 'neurons2': 27}\n",
      " mean=5.917, std=0.8343 using {'neurons1': 21, 'neurons2': 28}\n",
      " mean=4.983, std=0.3589 using {'neurons1': 21, 'neurons2': 29}\n",
      " mean=5.003, std=0.3767 using {'neurons1': 21, 'neurons2': 30}\n",
      " mean=7.094, std=1.67 using {'neurons1': 22, 'neurons2': 20}\n",
      " mean=8.292, std=1.161 using {'neurons1': 22, 'neurons2': 21}\n",
      " mean=5.067, std=0.5194 using {'neurons1': 22, 'neurons2': 22}\n",
      " mean=5.177, std=0.5863 using {'neurons1': 22, 'neurons2': 23}\n",
      " mean=6.369, std=2.238 using {'neurons1': 22, 'neurons2': 24}\n",
      " mean=5.805, std=1.526 using {'neurons1': 22, 'neurons2': 25}\n",
      " mean=5.983, std=1.247 using {'neurons1': 22, 'neurons2': 26}\n",
      " mean=6.438, std=1.389 using {'neurons1': 22, 'neurons2': 27}\n",
      " mean=5.808, std=0.4557 using {'neurons1': 22, 'neurons2': 28}\n",
      " mean=6.131, std=1.748 using {'neurons1': 22, 'neurons2': 29}\n",
      " mean=5.625, std=1.149 using {'neurons1': 22, 'neurons2': 30}\n",
      " mean=7.023, std=1.238 using {'neurons1': 23, 'neurons2': 20}\n",
      " mean=5.622, std=1.129 using {'neurons1': 23, 'neurons2': 21}\n",
      " mean=6.668, std=0.7156 using {'neurons1': 23, 'neurons2': 22}\n",
      " mean=6.791, std=1.034 using {'neurons1': 23, 'neurons2': 23}\n",
      " mean=5.319, std=0.4705 using {'neurons1': 23, 'neurons2': 24}\n",
      " mean=6.303, std=0.9191 using {'neurons1': 23, 'neurons2': 25}\n",
      " mean=6.162, std=0.8168 using {'neurons1': 23, 'neurons2': 26}\n",
      " mean=6.435, std=0.4452 using {'neurons1': 23, 'neurons2': 27}\n",
      " mean=5.823, std=0.9344 using {'neurons1': 23, 'neurons2': 28}\n",
      " mean=5.543, std=0.209 using {'neurons1': 23, 'neurons2': 29}\n",
      " mean=5.951, std=0.8227 using {'neurons1': 23, 'neurons2': 30}\n",
      " mean=7.965, std=1.322 using {'neurons1': 24, 'neurons2': 20}\n",
      " mean=7.732, std=0.7344 using {'neurons1': 24, 'neurons2': 21}\n",
      " mean=5.549, std=1.197 using {'neurons1': 24, 'neurons2': 22}\n",
      " mean=6.886, std=1.038 using {'neurons1': 24, 'neurons2': 23}\n",
      " mean=7.631, std=1.539 using {'neurons1': 24, 'neurons2': 24}\n",
      " mean=5.142, std=0.1813 using {'neurons1': 24, 'neurons2': 25}\n",
      " mean=6.482, std=0.7262 using {'neurons1': 24, 'neurons2': 26}\n",
      " mean=5.093, std=0.5073 using {'neurons1': 24, 'neurons2': 27}\n",
      " mean=5.829, std=0.3131 using {'neurons1': 24, 'neurons2': 28}\n",
      " mean=7.576, std=2.168 using {'neurons1': 24, 'neurons2': 29}\n",
      " mean=4.877, std=0.2039 using {'neurons1': 24, 'neurons2': 30}\n",
      " mean=5.76, std=0.2455 using {'neurons1': 25, 'neurons2': 20}\n",
      " mean=6.252, std=0.633 using {'neurons1': 25, 'neurons2': 21}\n",
      " mean=6.146, std=1.413 using {'neurons1': 25, 'neurons2': 22}\n",
      " mean=8.893, std=3.762 using {'neurons1': 25, 'neurons2': 23}\n",
      " mean=5.852, std=0.722 using {'neurons1': 25, 'neurons2': 24}\n",
      " mean=5.183, std=0.04352 using {'neurons1': 25, 'neurons2': 25}\n",
      " mean=6.667, std=0.5814 using {'neurons1': 25, 'neurons2': 26}\n",
      " mean=5.515, std=1.224 using {'neurons1': 25, 'neurons2': 27}\n",
      " mean=5.913, std=0.9992 using {'neurons1': 25, 'neurons2': 28}\n",
      " mean=5.879, std=0.9621 using {'neurons1': 25, 'neurons2': 29}\n",
      " mean=5.667, std=0.8187 using {'neurons1': 25, 'neurons2': 30}\n",
      " mean=5.5, std=0.5868 using {'neurons1': 26, 'neurons2': 20}\n",
      " mean=7.089, std=1.14 using {'neurons1': 26, 'neurons2': 21}\n",
      " mean=6.563, std=0.226 using {'neurons1': 26, 'neurons2': 22}\n",
      " mean=7.686, std=1.208 using {'neurons1': 26, 'neurons2': 23}\n",
      " mean=6.153, std=0.323 using {'neurons1': 26, 'neurons2': 24}\n",
      " mean=5.914, std=0.4324 using {'neurons1': 26, 'neurons2': 25}\n",
      " mean=7.423, std=1.762 using {'neurons1': 26, 'neurons2': 26}\n",
      " mean=5.312, std=0.2544 using {'neurons1': 26, 'neurons2': 27}\n",
      " mean=5.339, std=0.416 using {'neurons1': 26, 'neurons2': 28}\n",
      " mean=6.229, std=0.4408 using {'neurons1': 26, 'neurons2': 29}\n",
      " mean=4.804, std=0.1781 using {'neurons1': 26, 'neurons2': 30}\n",
      " mean=7.264, std=1.131 using {'neurons1': 27, 'neurons2': 20}\n",
      " mean=5.604, std=0.2072 using {'neurons1': 27, 'neurons2': 21}\n",
      " mean=5.554, std=0.5285 using {'neurons1': 27, 'neurons2': 22}\n",
      " mean=7.224, std=1.842 using {'neurons1': 27, 'neurons2': 23}\n",
      " mean=5.329, std=0.881 using {'neurons1': 27, 'neurons2': 24}\n",
      " mean=5.884, std=0.786 using {'neurons1': 27, 'neurons2': 25}\n",
      " mean=6.379, std=1.37 using {'neurons1': 27, 'neurons2': 26}\n",
      " mean=5.499, std=0.7994 using {'neurons1': 27, 'neurons2': 27}\n",
      " mean=5.848, std=0.7534 using {'neurons1': 27, 'neurons2': 28}\n",
      " mean=5.178, std=0.85 using {'neurons1': 27, 'neurons2': 29}\n",
      " mean=6.064, std=1.046 using {'neurons1': 27, 'neurons2': 30}\n",
      " mean=6.088, std=1.112 using {'neurons1': 28, 'neurons2': 20}\n",
      " mean=6.413, std=0.779 using {'neurons1': 28, 'neurons2': 21}\n",
      " mean=6.79, std=0.7053 using {'neurons1': 28, 'neurons2': 22}\n",
      " mean=7.505, std=2.595 using {'neurons1': 28, 'neurons2': 23}\n",
      " mean=4.845, std=0.4753 using {'neurons1': 28, 'neurons2': 24}\n",
      " mean=4.679, std=0.304 using {'neurons1': 28, 'neurons2': 25}\n",
      " mean=7.667, std=1.223 using {'neurons1': 28, 'neurons2': 26}\n",
      " mean=5.872, std=1.151 using {'neurons1': 28, 'neurons2': 27}\n",
      " mean=6.725, std=1.491 using {'neurons1': 28, 'neurons2': 28}\n",
      " mean=5.443, std=1.276 using {'neurons1': 28, 'neurons2': 29}\n",
      " mean=5.167, std=0.8556 using {'neurons1': 28, 'neurons2': 30}\n",
      " mean=6.296, std=1.281 using {'neurons1': 29, 'neurons2': 20}\n",
      " mean=6.74, std=1.195 using {'neurons1': 29, 'neurons2': 21}\n",
      " mean=5.904, std=0.776 using {'neurons1': 29, 'neurons2': 22}\n",
      " mean=5.519, std=1.392 using {'neurons1': 29, 'neurons2': 23}\n",
      " mean=6.738, std=0.9681 using {'neurons1': 29, 'neurons2': 24}\n",
      " mean=6.219, std=0.9023 using {'neurons1': 29, 'neurons2': 25}\n",
      " mean=6.057, std=0.7557 using {'neurons1': 29, 'neurons2': 26}\n",
      " mean=7.4, std=1.363 using {'neurons1': 29, 'neurons2': 27}\n",
      " mean=6.659, std=1.036 using {'neurons1': 29, 'neurons2': 28}\n",
      " mean=5.394, std=0.713 using {'neurons1': 29, 'neurons2': 29}\n",
      " mean=5.216, std=0.7751 using {'neurons1': 29, 'neurons2': 30}\n",
      " mean=6.587, std=0.7225 using {'neurons1': 30, 'neurons2': 20}\n",
      " mean=7.098, std=1.642 using {'neurons1': 30, 'neurons2': 21}\n",
      " mean=6.431, std=0.5791 using {'neurons1': 30, 'neurons2': 22}\n",
      " mean=5.688, std=0.6932 using {'neurons1': 30, 'neurons2': 23}\n",
      " mean=6.093, std=1.097 using {'neurons1': 30, 'neurons2': 24}\n",
      " mean=5.824, std=1.085 using {'neurons1': 30, 'neurons2': 25}\n",
      " mean=5.381, std=0.4823 using {'neurons1': 30, 'neurons2': 26}\n",
      " mean=5.98, std=0.6005 using {'neurons1': 30, 'neurons2': 27}\n",
      " mean=5.358, std=0.419 using {'neurons1': 30, 'neurons2': 28}\n",
      " mean=5.546, std=1.208 using {'neurons1': 30, 'neurons2': 29}\n",
      " mean=5.315, std=0.9379 using {'neurons1': 30, 'neurons2': 30}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Mape for {np.abs(grid_result.best_score_)} using {grid_result.best_params_}')\n",
    "means = np.abs(grid_result.cv_results_['mean_test_score'])\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, le nombre de neurone optimal serait 28 neurones dans la première couche et 25 dans la deuxième."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons donc implémenter notre modèle final \"model_F\" avec les paramètres sélectionnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 28)                252       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 25)                725       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 78        \n",
      "=================================================================\n",
      "Total params: 1,055\n",
      "Trainable params: 1,055\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_F = Sequential() #Initialisation modèle\n",
    "\n",
    "model_F.add(Dense(28, input_dim=8, \n",
    "                kernel_initializer='normal', \n",
    "                activation='relu'))\n",
    "\n",
    "#deuxième couche : 20 neurones\n",
    "model_F.add(Dense(25, kernel_initializer='normal', \n",
    "                activation='relu'))\n",
    "\n",
    "model_F.add(Dense(3, kernel_initializer='normal', activation='linear'))\n",
    "\n",
    "model_F.compile(loss='mean_absolute_percentage_error', \n",
    "              optimizer='Adamax', \n",
    "              metrics = ['mape']) #une ou plrs fonction pour évaluer le modèle)\n",
    "\n",
    "model_F.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/250\n",
      " - 2s - loss: 42.8992 - mape: 42.8992 - val_loss: 33.5448 - val_mape: 33.5448\n",
      "Epoch 2/250\n",
      " - 1s - loss: 28.4444 - mape: 28.4444 - val_loss: 20.7740 - val_mape: 20.7740\n",
      "Epoch 3/250\n",
      " - 1s - loss: 21.1954 - mape: 21.1954 - val_loss: 17.8944 - val_mape: 17.8944\n",
      "Epoch 4/250\n",
      " - 1s - loss: 19.5535 - mape: 19.5535 - val_loss: 17.5010 - val_mape: 17.5010\n",
      "Epoch 5/250\n",
      " - 1s - loss: 17.8398 - mape: 17.8398 - val_loss: 16.0137 - val_mape: 16.0137\n",
      "Epoch 6/250\n",
      " - 2s - loss: 14.4521 - mape: 14.4521 - val_loss: 12.5696 - val_mape: 12.5696\n",
      "Epoch 7/250\n",
      " - 1s - loss: 11.0918 - mape: 11.0918 - val_loss: 10.6879 - val_mape: 10.6879\n",
      "Epoch 8/250\n",
      " - 2s - loss: 10.2301 - mape: 10.2301 - val_loss: 7.8584 - val_mape: 7.8584\n",
      "Epoch 9/250\n",
      " - 2s - loss: 9.8826 - mape: 9.8826 - val_loss: 10.6545 - val_mape: 10.6545\n",
      "Epoch 10/250\n",
      " - 2s - loss: 9.2098 - mape: 9.2098 - val_loss: 8.1672 - val_mape: 8.1672\n",
      "Epoch 11/250\n",
      " - 1s - loss: 9.2365 - mape: 9.2365 - val_loss: 11.2629 - val_mape: 11.2628\n",
      "Epoch 12/250\n",
      " - 1s - loss: 8.6534 - mape: 8.6534 - val_loss: 7.7899 - val_mape: 7.7899\n",
      "Epoch 13/250\n",
      " - 1s - loss: 8.2695 - mape: 8.2695 - val_loss: 8.1079 - val_mape: 8.1079\n",
      "Epoch 14/250\n",
      " - 1s - loss: 7.9101 - mape: 7.9101 - val_loss: 6.9051 - val_mape: 6.9051\n",
      "Epoch 15/250\n",
      " - 1s - loss: 8.0153 - mape: 8.0153 - val_loss: 7.8322 - val_mape: 7.8322\n",
      "Epoch 16/250\n",
      " - 1s - loss: 7.5668 - mape: 7.5668 - val_loss: 5.3459 - val_mape: 5.3459\n",
      "Epoch 17/250\n",
      " - 2s - loss: 7.4386 - mape: 7.4386 - val_loss: 7.0361 - val_mape: 7.0361\n",
      "Epoch 18/250\n",
      " - 1s - loss: 7.3105 - mape: 7.3105 - val_loss: 6.9535 - val_mape: 6.9535\n",
      "Epoch 19/250\n",
      " - 2s - loss: 6.9422 - mape: 6.9422 - val_loss: 11.6345 - val_mape: 11.6345\n",
      "Epoch 20/250\n",
      " - 2s - loss: 7.3670 - mape: 7.3670 - val_loss: 5.0087 - val_mape: 5.0087\n",
      "Epoch 21/250\n",
      " - 1s - loss: 6.8939 - mape: 6.8939 - val_loss: 5.6767 - val_mape: 5.6767\n",
      "Epoch 22/250\n",
      " - 2s - loss: 6.6177 - mape: 6.6177 - val_loss: 4.8526 - val_mape: 4.8526\n",
      "Epoch 23/250\n",
      " - 1s - loss: 6.5633 - mape: 6.5633 - val_loss: 4.6342 - val_mape: 4.6342\n",
      "Epoch 24/250\n",
      " - 1s - loss: 6.6442 - mape: 6.6442 - val_loss: 6.0762 - val_mape: 6.0762\n",
      "Epoch 25/250\n",
      " - 1s - loss: 6.5152 - mape: 6.5152 - val_loss: 5.4127 - val_mape: 5.4127\n",
      "Epoch 26/250\n",
      " - 1s - loss: 6.2977 - mape: 6.2977 - val_loss: 8.5740 - val_mape: 8.5740\n",
      "Epoch 27/250\n",
      " - 1s - loss: 6.1246 - mape: 6.1246 - val_loss: 5.7447 - val_mape: 5.7447\n",
      "Epoch 28/250\n",
      " - 2s - loss: 6.2169 - mape: 6.2169 - val_loss: 4.9155 - val_mape: 4.9155\n",
      "Epoch 29/250\n",
      " - 1s - loss: 6.0944 - mape: 6.0945 - val_loss: 5.0089 - val_mape: 5.0089\n",
      "Epoch 30/250\n",
      " - 2s - loss: 6.3049 - mape: 6.3049 - val_loss: 6.0422 - val_mape: 6.0422\n",
      "Epoch 31/250\n",
      " - 2s - loss: 6.2823 - mape: 6.2823 - val_loss: 7.1657 - val_mape: 7.1657\n",
      "Epoch 32/250\n",
      " - 1s - loss: 5.9941 - mape: 5.9941 - val_loss: 5.4720 - val_mape: 5.4720\n",
      "Epoch 33/250\n",
      " - 2s - loss: 5.9299 - mape: 5.9299 - val_loss: 4.7998 - val_mape: 4.7998\n",
      "Epoch 34/250\n",
      " - 2s - loss: 6.1715 - mape: 6.1715 - val_loss: 6.3948 - val_mape: 6.3948\n",
      "Epoch 35/250\n",
      " - 1s - loss: 6.3574 - mape: 6.3574 - val_loss: 6.0347 - val_mape: 6.0347\n",
      "Epoch 36/250\n",
      " - 1s - loss: 6.1169 - mape: 6.1169 - val_loss: 9.1739 - val_mape: 9.1739\n",
      "Epoch 37/250\n",
      " - 1s - loss: 6.3427 - mape: 6.3427 - val_loss: 4.6082 - val_mape: 4.6082\n",
      "Epoch 38/250\n",
      " - 2s - loss: 5.8365 - mape: 5.8365 - val_loss: 7.7642 - val_mape: 7.7642\n",
      "Epoch 39/250\n",
      " - 2s - loss: 5.9230 - mape: 5.9230 - val_loss: 4.5669 - val_mape: 4.5669\n",
      "Epoch 40/250\n",
      " - 2s - loss: 5.8140 - mape: 5.8140 - val_loss: 7.5495 - val_mape: 7.5495\n",
      "Epoch 41/250\n",
      " - 2s - loss: 5.6005 - mape: 5.6005 - val_loss: 4.4806 - val_mape: 4.4806\n",
      "Epoch 42/250\n",
      " - 1s - loss: 6.0217 - mape: 6.0217 - val_loss: 5.7970 - val_mape: 5.7970\n",
      "Epoch 43/250\n",
      " - 1s - loss: 5.7603 - mape: 5.7603 - val_loss: 5.1281 - val_mape: 5.1281\n",
      "Epoch 44/250\n",
      " - 2s - loss: 5.7694 - mape: 5.7694 - val_loss: 6.2808 - val_mape: 6.2808\n",
      "Epoch 45/250\n",
      " - 1s - loss: 5.5997 - mape: 5.5997 - val_loss: 5.1393 - val_mape: 5.1393\n",
      "Epoch 46/250\n",
      " - 1s - loss: 5.6063 - mape: 5.6063 - val_loss: 4.6767 - val_mape: 4.6767\n",
      "Epoch 47/250\n",
      " - 1s - loss: 5.4445 - mape: 5.4445 - val_loss: 4.2033 - val_mape: 4.2033\n",
      "Epoch 48/250\n",
      " - 1s - loss: 5.5197 - mape: 5.5197 - val_loss: 4.8009 - val_mape: 4.8009\n",
      "Epoch 49/250\n",
      " - 1s - loss: 5.5613 - mape: 5.5613 - val_loss: 10.1024 - val_mape: 10.1024\n",
      "Epoch 50/250\n",
      " - 2s - loss: 5.4654 - mape: 5.4654 - val_loss: 5.3898 - val_mape: 5.3898\n",
      "Epoch 51/250\n",
      " - 2s - loss: 5.5313 - mape: 5.5313 - val_loss: 5.6261 - val_mape: 5.6261\n",
      "Epoch 52/250\n",
      " - 1s - loss: 5.2630 - mape: 5.2630 - val_loss: 5.0305 - val_mape: 5.0305\n",
      "Epoch 53/250\n",
      " - 1s - loss: 5.2893 - mape: 5.2893 - val_loss: 4.8411 - val_mape: 4.8411\n",
      "Epoch 54/250\n",
      " - 1s - loss: 5.5084 - mape: 5.5084 - val_loss: 3.7302 - val_mape: 3.7302\n",
      "Epoch 55/250\n",
      " - 2s - loss: 5.4955 - mape: 5.4955 - val_loss: 4.1734 - val_mape: 4.1734\n",
      "Epoch 56/250\n",
      " - 1s - loss: 5.4428 - mape: 5.4428 - val_loss: 4.0656 - val_mape: 4.0656\n",
      "Epoch 57/250\n",
      " - 1s - loss: 5.2593 - mape: 5.2593 - val_loss: 4.3006 - val_mape: 4.3006\n",
      "Epoch 58/250\n",
      " - 2s - loss: 5.2247 - mape: 5.2247 - val_loss: 4.5880 - val_mape: 4.5880\n",
      "Epoch 59/250\n",
      " - 2s - loss: 5.2345 - mape: 5.2345 - val_loss: 9.1054 - val_mape: 9.1054\n",
      "Epoch 60/250\n",
      " - 2s - loss: 5.2345 - mape: 5.2345 - val_loss: 4.9290 - val_mape: 4.9290\n",
      "Epoch 61/250\n",
      " - 2s - loss: 5.1478 - mape: 5.1478 - val_loss: 4.5325 - val_mape: 4.5325\n",
      "Epoch 62/250\n",
      " - 2s - loss: 5.1665 - mape: 5.1665 - val_loss: 5.3654 - val_mape: 5.3654\n",
      "Epoch 63/250\n",
      " - 1s - loss: 4.9617 - mape: 4.9617 - val_loss: 3.6508 - val_mape: 3.6508\n",
      "Epoch 64/250\n",
      " - 1s - loss: 5.3398 - mape: 5.3398 - val_loss: 5.5012 - val_mape: 5.5012\n",
      "Epoch 65/250\n",
      " - 2s - loss: 4.9603 - mape: 4.9603 - val_loss: 6.3591 - val_mape: 6.3591\n",
      "Epoch 66/250\n",
      " - 1s - loss: 4.8712 - mape: 4.8712 - val_loss: 7.5960 - val_mape: 7.5960\n",
      "Epoch 67/250\n",
      " - 2s - loss: 4.8551 - mape: 4.8551 - val_loss: 5.9658 - val_mape: 5.9658\n",
      "Epoch 68/250\n",
      " - 2s - loss: 5.0342 - mape: 5.0342 - val_loss: 6.4256 - val_mape: 6.4256\n",
      "Epoch 69/250\n",
      " - 2s - loss: 5.0706 - mape: 5.0706 - val_loss: 6.9349 - val_mape: 6.9349\n",
      "Epoch 70/250\n",
      " - 1s - loss: 4.7496 - mape: 4.7496 - val_loss: 3.9537 - val_mape: 3.9537\n",
      "Epoch 71/250\n",
      " - 2s - loss: 4.8342 - mape: 4.8342 - val_loss: 4.5374 - val_mape: 4.5374\n",
      "Epoch 72/250\n",
      " - 2s - loss: 4.8146 - mape: 4.8146 - val_loss: 4.3946 - val_mape: 4.3946\n",
      "Epoch 73/250\n",
      " - 1s - loss: 4.9379 - mape: 4.9379 - val_loss: 3.4249 - val_mape: 3.4249\n",
      "Epoch 74/250\n",
      " - 1s - loss: 4.8503 - mape: 4.8503 - val_loss: 5.0827 - val_mape: 5.0827\n",
      "Epoch 75/250\n",
      " - 1s - loss: 4.9249 - mape: 4.9249 - val_loss: 3.6699 - val_mape: 3.6699\n",
      "Epoch 76/250\n",
      " - 2s - loss: 4.5956 - mape: 4.5956 - val_loss: 4.4095 - val_mape: 4.4095\n",
      "Epoch 77/250\n",
      " - 2s - loss: 4.8484 - mape: 4.8484 - val_loss: 3.9915 - val_mape: 3.9915\n",
      "Epoch 78/250\n",
      " - 2s - loss: 4.8207 - mape: 4.8207 - val_loss: 5.1737 - val_mape: 5.1737\n",
      "Epoch 79/250\n",
      " - 1s - loss: 4.7023 - mape: 4.7023 - val_loss: 5.7549 - val_mape: 5.7549\n",
      "Epoch 80/250\n",
      " - 1s - loss: 4.6167 - mape: 4.6167 - val_loss: 9.0276 - val_mape: 9.0276\n",
      "Epoch 81/250\n",
      " - 2s - loss: 4.6866 - mape: 4.6866 - val_loss: 5.5329 - val_mape: 5.5329\n",
      "Epoch 82/250\n",
      " - 2s - loss: 4.9061 - mape: 4.9061 - val_loss: 3.9484 - val_mape: 3.9484\n",
      "Epoch 83/250\n",
      " - 1s - loss: 4.7135 - mape: 4.7135 - val_loss: 3.1064 - val_mape: 3.1064\n",
      "Epoch 84/250\n",
      " - 1s - loss: 4.9690 - mape: 4.9690 - val_loss: 6.2891 - val_mape: 6.2891\n",
      "Epoch 85/250\n",
      " - 1s - loss: 4.7459 - mape: 4.7459 - val_loss: 4.3253 - val_mape: 4.3253\n",
      "Epoch 86/250\n",
      " - 1s - loss: 4.7480 - mape: 4.7480 - val_loss: 3.1593 - val_mape: 3.1593\n",
      "Epoch 87/250\n",
      " - 1s - loss: 4.6642 - mape: 4.6642 - val_loss: 3.6664 - val_mape: 3.6664\n",
      "Epoch 88/250\n",
      " - 1s - loss: 4.6981 - mape: 4.6981 - val_loss: 5.2671 - val_mape: 5.2671\n",
      "Epoch 89/250\n",
      " - 1s - loss: 4.6331 - mape: 4.6331 - val_loss: 3.4220 - val_mape: 3.4220\n",
      "Epoch 90/250\n",
      " - 1s - loss: 4.6637 - mape: 4.6637 - val_loss: 4.2487 - val_mape: 4.2487\n",
      "Epoch 91/250\n",
      " - 1s - loss: 4.5609 - mape: 4.5609 - val_loss: 4.7124 - val_mape: 4.7124\n",
      "Epoch 92/250\n",
      " - 2s - loss: 4.9637 - mape: 4.9637 - val_loss: 5.4015 - val_mape: 5.4015\n",
      "Epoch 93/250\n",
      " - 2s - loss: 4.5912 - mape: 4.5912 - val_loss: 3.8917 - val_mape: 3.8917\n",
      "Epoch 94/250\n",
      " - 1s - loss: 4.6130 - mape: 4.6130 - val_loss: 3.8580 - val_mape: 3.8580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/250\n",
      " - 1s - loss: 4.4682 - mape: 4.4682 - val_loss: 6.1066 - val_mape: 6.1066\n",
      "Epoch 96/250\n",
      " - 2s - loss: 4.5064 - mape: 4.5064 - val_loss: 4.5035 - val_mape: 4.5035\n",
      "Epoch 97/250\n",
      " - 2s - loss: 4.4700 - mape: 4.4700 - val_loss: 4.0197 - val_mape: 4.0197\n",
      "Epoch 98/250\n",
      " - 2s - loss: 4.5064 - mape: 4.5064 - val_loss: 7.0230 - val_mape: 7.0230\n",
      "Epoch 99/250\n",
      " - 1s - loss: 4.5688 - mape: 4.5688 - val_loss: 3.9110 - val_mape: 3.9110\n",
      "Epoch 100/250\n",
      " - 1s - loss: 4.8550 - mape: 4.8550 - val_loss: 4.3769 - val_mape: 4.3769\n",
      "Epoch 101/250\n",
      " - 1s - loss: 4.3914 - mape: 4.3914 - val_loss: 4.2077 - val_mape: 4.2077\n",
      "Epoch 102/250\n",
      " - 2s - loss: 4.5850 - mape: 4.5850 - val_loss: 5.2569 - val_mape: 5.2569\n",
      "Epoch 103/250\n",
      " - 2s - loss: 4.4709 - mape: 4.4709 - val_loss: 4.3804 - val_mape: 4.3804\n",
      "Epoch 104/250\n",
      " - 1s - loss: 4.5677 - mape: 4.5677 - val_loss: 3.6470 - val_mape: 3.6470\n",
      "Epoch 105/250\n",
      " - 1s - loss: 4.5245 - mape: 4.5245 - val_loss: 4.8036 - val_mape: 4.8036\n",
      "Epoch 106/250\n",
      " - 1s - loss: 4.4946 - mape: 4.4946 - val_loss: 3.5453 - val_mape: 3.5453\n",
      "Epoch 107/250\n",
      " - 1s - loss: 4.2178 - mape: 4.2178 - val_loss: 5.3331 - val_mape: 5.3331\n",
      "Epoch 108/250\n",
      " - 2s - loss: 4.4102 - mape: 4.4102 - val_loss: 6.0235 - val_mape: 6.0235\n",
      "Epoch 109/250\n",
      " - 1s - loss: 4.4504 - mape: 4.4504 - val_loss: 4.5558 - val_mape: 4.5558\n",
      "Epoch 110/250\n",
      " - 1s - loss: 4.3451 - mape: 4.3451 - val_loss: 3.6814 - val_mape: 3.6814\n",
      "Epoch 111/250\n",
      " - 1s - loss: 4.5265 - mape: 4.5265 - val_loss: 5.0173 - val_mape: 5.0173\n",
      "Epoch 112/250\n",
      " - 1s - loss: 4.3580 - mape: 4.3580 - val_loss: 3.1986 - val_mape: 3.1986\n",
      "Epoch 113/250\n",
      " - 2s - loss: 4.5365 - mape: 4.5365 - val_loss: 3.9284 - val_mape: 3.9284\n",
      "Epoch 114/250\n",
      " - 2s - loss: 4.2838 - mape: 4.2838 - val_loss: 6.7932 - val_mape: 6.7932\n",
      "Epoch 115/250\n",
      " - 1s - loss: 4.3210 - mape: 4.3210 - val_loss: 3.0357 - val_mape: 3.0357\n",
      "Epoch 116/250\n",
      " - 2s - loss: 4.4347 - mape: 4.4347 - val_loss: 4.3026 - val_mape: 4.3026\n",
      "Epoch 117/250\n",
      " - 2s - loss: 4.3982 - mape: 4.3982 - val_loss: 5.1132 - val_mape: 5.1132\n",
      "Epoch 118/250\n",
      " - 1s - loss: 4.2369 - mape: 4.2369 - val_loss: 2.8547 - val_mape: 2.8547\n",
      "Epoch 119/250\n",
      " - 2s - loss: 4.5623 - mape: 4.5623 - val_loss: 8.2729 - val_mape: 8.2729\n",
      "Epoch 120/250\n",
      " - 1s - loss: 4.3947 - mape: 4.3947 - val_loss: 3.4209 - val_mape: 3.4209\n",
      "Epoch 121/250\n",
      " - 1s - loss: 4.4771 - mape: 4.4771 - val_loss: 3.1907 - val_mape: 3.1907\n",
      "Epoch 122/250\n",
      " - 1s - loss: 4.3676 - mape: 4.3676 - val_loss: 4.0397 - val_mape: 4.0397\n",
      "Epoch 123/250\n",
      " - 2s - loss: 4.2179 - mape: 4.2179 - val_loss: 5.5143 - val_mape: 5.5143\n",
      "Epoch 124/250\n",
      " - 2s - loss: 4.2702 - mape: 4.2702 - val_loss: 3.6260 - val_mape: 3.6260\n",
      "Epoch 125/250\n",
      " - 1s - loss: 4.0402 - mape: 4.0402 - val_loss: 3.9455 - val_mape: 3.9455\n",
      "Epoch 126/250\n",
      " - 1s - loss: 4.4870 - mape: 4.4870 - val_loss: 3.6874 - val_mape: 3.6874\n",
      "Epoch 127/250\n",
      " - 1s - loss: 4.3359 - mape: 4.3359 - val_loss: 3.6475 - val_mape: 3.6475\n",
      "Epoch 128/250\n",
      " - 1s - loss: 4.4051 - mape: 4.4051 - val_loss: 3.9383 - val_mape: 3.9383\n",
      "Epoch 129/250\n",
      " - 1s - loss: 4.0467 - mape: 4.0467 - val_loss: 5.3402 - val_mape: 5.3402\n",
      "Epoch 130/250\n",
      " - 2s - loss: 4.3501 - mape: 4.3501 - val_loss: 5.6872 - val_mape: 5.6872\n",
      "Epoch 131/250\n",
      " - 1s - loss: 4.2716 - mape: 4.2716 - val_loss: 3.3693 - val_mape: 3.3693\n",
      "Epoch 132/250\n",
      " - 1s - loss: 3.9801 - mape: 3.9801 - val_loss: 5.6103 - val_mape: 5.6103\n",
      "Epoch 133/250\n",
      " - 1s - loss: 4.3469 - mape: 4.3469 - val_loss: 5.5234 - val_mape: 5.5234\n",
      "Epoch 134/250\n",
      " - 2s - loss: 4.0414 - mape: 4.0414 - val_loss: 3.0943 - val_mape: 3.0943\n",
      "Epoch 135/250\n",
      " - 2s - loss: 3.9887 - mape: 3.9887 - val_loss: 3.9629 - val_mape: 3.9629\n",
      "Epoch 136/250\n",
      " - 2s - loss: 4.2298 - mape: 4.2298 - val_loss: 6.0467 - val_mape: 6.0467\n",
      "Epoch 137/250\n",
      " - 2s - loss: 4.1078 - mape: 4.1078 - val_loss: 5.3489 - val_mape: 5.3489\n",
      "Epoch 138/250\n",
      " - 1s - loss: 4.0379 - mape: 4.0379 - val_loss: 4.5272 - val_mape: 4.5272\n",
      "Epoch 139/250\n",
      " - 1s - loss: 4.4101 - mape: 4.4101 - val_loss: 4.0386 - val_mape: 4.0386\n",
      "Epoch 140/250\n",
      " - 1s - loss: 4.1633 - mape: 4.1633 - val_loss: 3.7683 - val_mape: 3.7683\n",
      "Epoch 141/250\n",
      " - 2s - loss: 4.2005 - mape: 4.2005 - val_loss: 4.0421 - val_mape: 4.0421\n",
      "Epoch 142/250\n",
      " - 1s - loss: 4.0709 - mape: 4.0709 - val_loss: 4.6983 - val_mape: 4.6983\n",
      "Epoch 143/250\n",
      " - 1s - loss: 4.1007 - mape: 4.1007 - val_loss: 3.7328 - val_mape: 3.7328\n",
      "Epoch 144/250\n",
      " - 2s - loss: 4.0785 - mape: 4.0785 - val_loss: 4.6716 - val_mape: 4.6716\n",
      "Epoch 145/250\n",
      " - 2s - loss: 4.0534 - mape: 4.0534 - val_loss: 3.1069 - val_mape: 3.1069\n",
      "Epoch 146/250\n",
      " - 2s - loss: 4.1835 - mape: 4.1835 - val_loss: 3.7692 - val_mape: 3.7692\n",
      "Epoch 147/250\n",
      " - 1s - loss: 4.1557 - mape: 4.1557 - val_loss: 6.3567 - val_mape: 6.3567\n",
      "Epoch 148/250\n",
      " - 1s - loss: 4.2136 - mape: 4.2136 - val_loss: 5.9295 - val_mape: 5.9295\n",
      "Epoch 149/250\n",
      " - 1s - loss: 4.2712 - mape: 4.2712 - val_loss: 4.2145 - val_mape: 4.2146\n",
      "Epoch 150/250\n",
      " - 1s - loss: 4.1522 - mape: 4.1522 - val_loss: 5.2709 - val_mape: 5.2709\n",
      "Epoch 151/250\n",
      " - 1s - loss: 4.2057 - mape: 4.2057 - val_loss: 7.6468 - val_mape: 7.6468\n",
      "Epoch 152/250\n",
      " - 2s - loss: 4.1343 - mape: 4.1343 - val_loss: 3.8345 - val_mape: 3.8345\n",
      "Epoch 153/250\n",
      " - 1s - loss: 3.9935 - mape: 3.9935 - val_loss: 5.2817 - val_mape: 5.2817\n",
      "Epoch 154/250\n",
      " - 2s - loss: 4.1214 - mape: 4.1214 - val_loss: 3.4498 - val_mape: 3.4498\n",
      "Epoch 155/250\n",
      " - 2s - loss: 4.2240 - mape: 4.2240 - val_loss: 3.4254 - val_mape: 3.4254\n",
      "Epoch 156/250\n",
      " - 2s - loss: 4.1065 - mape: 4.1065 - val_loss: 3.6519 - val_mape: 3.6519\n",
      "Epoch 157/250\n",
      " - 2s - loss: 3.8861 - mape: 3.8861 - val_loss: 3.9656 - val_mape: 3.9656\n",
      "Epoch 158/250\n",
      " - 2s - loss: 3.9431 - mape: 3.9431 - val_loss: 3.1662 - val_mape: 3.1662\n",
      "Epoch 159/250\n",
      " - 1s - loss: 4.3169 - mape: 4.3169 - val_loss: 2.8907 - val_mape: 2.8907\n",
      "Epoch 160/250\n",
      " - 1s - loss: 4.1065 - mape: 4.1065 - val_loss: 4.2389 - val_mape: 4.2389\n",
      "Epoch 161/250\n",
      " - 1s - loss: 3.8634 - mape: 3.8634 - val_loss: 5.2270 - val_mape: 5.2270\n",
      "Epoch 162/250\n",
      " - 2s - loss: 4.1034 - mape: 4.1034 - val_loss: 3.9728 - val_mape: 3.9728\n",
      "Epoch 163/250\n",
      " - 1s - loss: 3.9596 - mape: 3.9596 - val_loss: 3.6540 - val_mape: 3.6540\n",
      "Epoch 164/250\n",
      " - 1s - loss: 4.0086 - mape: 4.0086 - val_loss: 3.4195 - val_mape: 3.4195\n",
      "Epoch 165/250\n",
      " - 2s - loss: 3.9942 - mape: 3.9942 - val_loss: 6.1013 - val_mape: 6.1013\n",
      "Epoch 166/250\n",
      " - 2s - loss: 4.1437 - mape: 4.1437 - val_loss: 3.1482 - val_mape: 3.1482\n",
      "Epoch 167/250\n",
      " - 1s - loss: 4.0811 - mape: 4.0811 - val_loss: 4.3177 - val_mape: 4.3177\n",
      "Epoch 168/250\n",
      " - 1s - loss: 3.9516 - mape: 3.9516 - val_loss: 3.4146 - val_mape: 3.4146\n",
      "Epoch 169/250\n",
      " - 1s - loss: 4.0438 - mape: 4.0438 - val_loss: 3.3778 - val_mape: 3.3778\n",
      "Epoch 170/250\n",
      " - 1s - loss: 3.9091 - mape: 3.9091 - val_loss: 4.2491 - val_mape: 4.2491\n",
      "Epoch 171/250\n",
      " - 1s - loss: 4.3084 - mape: 4.3084 - val_loss: 4.7999 - val_mape: 4.7999\n",
      "Epoch 172/250\n",
      " - 1s - loss: 4.0060 - mape: 4.0060 - val_loss: 3.6391 - val_mape: 3.6391\n",
      "Epoch 173/250\n",
      " - 2s - loss: 3.8945 - mape: 3.8945 - val_loss: 3.8076 - val_mape: 3.8076\n",
      "Epoch 174/250\n",
      " - 1s - loss: 3.9685 - mape: 3.9685 - val_loss: 3.6119 - val_mape: 3.6119\n",
      "Epoch 175/250\n",
      " - 2s - loss: 3.9524 - mape: 3.9524 - val_loss: 4.1132 - val_mape: 4.1132\n",
      "Epoch 176/250\n",
      " - 2s - loss: 3.8724 - mape: 3.8724 - val_loss: 3.3264 - val_mape: 3.3264\n",
      "Epoch 177/250\n",
      " - 1s - loss: 3.9425 - mape: 3.9425 - val_loss: 3.0614 - val_mape: 3.0614\n",
      "Epoch 178/250\n",
      " - 2s - loss: 3.9131 - mape: 3.9131 - val_loss: 4.2481 - val_mape: 4.2481\n",
      "Epoch 179/250\n",
      " - 1s - loss: 4.2219 - mape: 4.2219 - val_loss: 7.3561 - val_mape: 7.3561\n",
      "Epoch 180/250\n",
      " - 1s - loss: 4.0137 - mape: 4.0137 - val_loss: 3.6275 - val_mape: 3.6275\n",
      "Epoch 181/250\n",
      " - 1s - loss: 3.7597 - mape: 3.7597 - val_loss: 2.9519 - val_mape: 2.9519\n",
      "Epoch 182/250\n",
      " - 1s - loss: 3.8673 - mape: 3.8673 - val_loss: 3.1193 - val_mape: 3.1193\n",
      "Epoch 183/250\n",
      " - 1s - loss: 4.0060 - mape: 4.0060 - val_loss: 4.4860 - val_mape: 4.4860\n",
      "Epoch 184/250\n",
      " - 2s - loss: 3.9067 - mape: 3.9067 - val_loss: 4.7704 - val_mape: 4.7704\n",
      "Epoch 185/250\n",
      " - 2s - loss: 3.9325 - mape: 3.9325 - val_loss: 5.1832 - val_mape: 5.1832\n",
      "Epoch 186/250\n",
      " - 2s - loss: 4.0429 - mape: 4.0429 - val_loss: 6.0028 - val_mape: 6.0028\n",
      "Epoch 187/250\n",
      " - 2s - loss: 3.8688 - mape: 3.8688 - val_loss: 3.4309 - val_mape: 3.4309\n",
      "Epoch 188/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: 3.9783 - mape: 3.9783 - val_loss: 3.6965 - val_mape: 3.6965\n",
      "Epoch 189/250\n",
      " - 2s - loss: 3.9388 - mape: 3.9388 - val_loss: 3.2615 - val_mape: 3.2615\n",
      "Epoch 190/250\n",
      " - 1s - loss: 4.0497 - mape: 4.0497 - val_loss: 3.8644 - val_mape: 3.8644\n",
      "Epoch 191/250\n",
      " - 1s - loss: 3.7496 - mape: 3.7496 - val_loss: 2.9769 - val_mape: 2.9769\n",
      "Epoch 192/250\n",
      " - 1s - loss: 3.9213 - mape: 3.9213 - val_loss: 5.2564 - val_mape: 5.2564\n",
      "Epoch 193/250\n",
      " - 1s - loss: 3.9353 - mape: 3.9353 - val_loss: 3.1717 - val_mape: 3.1717\n",
      "Epoch 194/250\n",
      " - 1s - loss: 3.9847 - mape: 3.9847 - val_loss: 2.8882 - val_mape: 2.8882\n",
      "Epoch 195/250\n",
      " - 2s - loss: 4.0087 - mape: 4.0087 - val_loss: 3.6313 - val_mape: 3.6313\n",
      "Epoch 196/250\n",
      " - 2s - loss: 3.7954 - mape: 3.7954 - val_loss: 7.4562 - val_mape: 7.4562\n",
      "Epoch 197/250\n",
      " - 2s - loss: 3.8382 - mape: 3.8382 - val_loss: 3.4147 - val_mape: 3.4147\n",
      "Epoch 198/250\n",
      " - 2s - loss: 3.8621 - mape: 3.8621 - val_loss: 2.5276 - val_mape: 2.5276\n",
      "Epoch 199/250\n",
      " - 2s - loss: 3.9649 - mape: 3.9649 - val_loss: 3.4198 - val_mape: 3.4198\n",
      "Epoch 200/250\n",
      " - 1s - loss: 3.8776 - mape: 3.8776 - val_loss: 3.7023 - val_mape: 3.7023\n",
      "Epoch 201/250\n",
      " - 1s - loss: 3.8631 - mape: 3.8631 - val_loss: 3.8750 - val_mape: 3.8750\n",
      "Epoch 202/250\n",
      " - 1s - loss: 3.8362 - mape: 3.8362 - val_loss: 4.1692 - val_mape: 4.1692\n",
      "Epoch 203/250\n",
      " - 1s - loss: 3.8440 - mape: 3.8440 - val_loss: 6.4228 - val_mape: 6.4228\n",
      "Epoch 204/250\n",
      " - 1s - loss: 3.8899 - mape: 3.8899 - val_loss: 3.3142 - val_mape: 3.3142\n",
      "Epoch 205/250\n",
      " - 2s - loss: 3.8361 - mape: 3.8361 - val_loss: 4.9093 - val_mape: 4.9093\n",
      "Epoch 206/250\n",
      " - 2s - loss: 3.9386 - mape: 3.9386 - val_loss: 4.7662 - val_mape: 4.7662\n",
      "Epoch 207/250\n",
      " - 2s - loss: 4.0558 - mape: 4.0558 - val_loss: 5.3534 - val_mape: 5.3534\n",
      "Epoch 208/250\n",
      " - 1s - loss: 3.7877 - mape: 3.7877 - val_loss: 4.5685 - val_mape: 4.5685\n",
      "Epoch 209/250\n",
      " - 1s - loss: 3.7090 - mape: 3.7090 - val_loss: 3.5321 - val_mape: 3.5321\n",
      "Epoch 210/250\n",
      " - 2s - loss: 4.0423 - mape: 4.0423 - val_loss: 3.1234 - val_mape: 3.1234\n",
      "Epoch 211/250\n",
      " - 1s - loss: 3.7851 - mape: 3.7851 - val_loss: 5.1484 - val_mape: 5.1484\n",
      "Epoch 212/250\n",
      " - 1s - loss: 3.7383 - mape: 3.7383 - val_loss: 3.4214 - val_mape: 3.4214\n",
      "Epoch 213/250\n",
      " - 1s - loss: 3.9669 - mape: 3.9669 - val_loss: 3.8243 - val_mape: 3.8242\n",
      "Epoch 214/250\n",
      " - 2s - loss: 4.0539 - mape: 4.0539 - val_loss: 3.4677 - val_mape: 3.4677\n",
      "Epoch 215/250\n",
      " - 2s - loss: 3.8228 - mape: 3.8228 - val_loss: 2.9993 - val_mape: 2.9993\n",
      "Epoch 216/250\n",
      " - 2s - loss: 4.0558 - mape: 4.0558 - val_loss: 2.7299 - val_mape: 2.7299\n",
      "Epoch 217/250\n",
      " - 2s - loss: 3.8959 - mape: 3.8959 - val_loss: 3.4538 - val_mape: 3.4538\n",
      "Epoch 218/250\n",
      " - 1s - loss: 3.8730 - mape: 3.8730 - val_loss: 3.2562 - val_mape: 3.2562\n",
      "Epoch 219/250\n",
      " - 1s - loss: 4.1024 - mape: 4.1024 - val_loss: 3.3591 - val_mape: 3.3591\n",
      "Epoch 220/250\n",
      " - 1s - loss: 3.8700 - mape: 3.8700 - val_loss: 2.8301 - val_mape: 2.8301\n",
      "Epoch 221/250\n",
      " - 2s - loss: 3.6971 - mape: 3.6971 - val_loss: 3.5239 - val_mape: 3.5239\n",
      "Epoch 222/250\n",
      " - 1s - loss: 3.7946 - mape: 3.7946 - val_loss: 3.0809 - val_mape: 3.0809\n",
      "Epoch 223/250\n",
      " - 1s - loss: 3.9598 - mape: 3.9598 - val_loss: 3.7162 - val_mape: 3.7162\n",
      "Epoch 224/250\n",
      " - 1s - loss: 3.8462 - mape: 3.8462 - val_loss: 3.0942 - val_mape: 3.0942\n",
      "Epoch 225/250\n",
      " - 1s - loss: 3.9507 - mape: 3.9507 - val_loss: 3.2463 - val_mape: 3.2463\n",
      "Epoch 226/250\n",
      " - 1s - loss: 3.9499 - mape: 3.9499 - val_loss: 4.2990 - val_mape: 4.2990\n",
      "Epoch 227/250\n",
      " - 2s - loss: 3.8384 - mape: 3.8384 - val_loss: 3.7411 - val_mape: 3.7411\n",
      "Epoch 228/250\n",
      " - 2s - loss: 3.8522 - mape: 3.8522 - val_loss: 3.4913 - val_mape: 3.4913\n",
      "Epoch 229/250\n",
      " - 2s - loss: 3.8914 - mape: 3.8914 - val_loss: 3.3614 - val_mape: 3.3614\n",
      "Epoch 230/250\n",
      " - 1s - loss: 3.7541 - mape: 3.7541 - val_loss: 4.2379 - val_mape: 4.2379\n",
      "Epoch 231/250\n",
      " - 2s - loss: 3.6646 - mape: 3.6646 - val_loss: 4.8721 - val_mape: 4.8721\n",
      "Epoch 232/250\n",
      " - 1s - loss: 3.9438 - mape: 3.9438 - val_loss: 2.9864 - val_mape: 2.9864\n",
      "Epoch 233/250\n",
      " - 2s - loss: 3.9426 - mape: 3.9426 - val_loss: 4.0880 - val_mape: 4.0880\n",
      "Epoch 234/250\n",
      " - 2s - loss: 3.6944 - mape: 3.6944 - val_loss: 2.9500 - val_mape: 2.9500\n",
      "Epoch 235/250\n",
      " - 1s - loss: 3.9578 - mape: 3.9578 - val_loss: 4.3119 - val_mape: 4.3119\n",
      "Epoch 236/250\n",
      " - 1s - loss: 3.7535 - mape: 3.7535 - val_loss: 3.9444 - val_mape: 3.9444\n",
      "Epoch 237/250\n",
      " - 2s - loss: 3.8664 - mape: 3.8664 - val_loss: 3.5377 - val_mape: 3.5377\n",
      "Epoch 238/250\n",
      " - 2s - loss: 3.8167 - mape: 3.8167 - val_loss: 3.0616 - val_mape: 3.0616\n",
      "Epoch 239/250\n",
      " - 1s - loss: 3.7376 - mape: 3.7376 - val_loss: 5.6840 - val_mape: 5.6840\n",
      "Epoch 240/250\n",
      " - 1s - loss: 3.6345 - mape: 3.6345 - val_loss: 4.4011 - val_mape: 4.4011\n",
      "Epoch 241/250\n",
      " - 1s - loss: 3.6964 - mape: 3.6964 - val_loss: 3.7656 - val_mape: 3.7656\n",
      "Epoch 242/250\n",
      " - 2s - loss: 3.6916 - mape: 3.6916 - val_loss: 5.0759 - val_mape: 5.0759\n",
      "Epoch 243/250\n",
      " - 2s - loss: 3.7844 - mape: 3.7844 - val_loss: 2.6001 - val_mape: 2.6001\n",
      "Epoch 244/250\n",
      " - 2s - loss: 3.7188 - mape: 3.7188 - val_loss: 3.8440 - val_mape: 3.8440\n",
      "Epoch 245/250\n",
      " - 1s - loss: 3.8356 - mape: 3.8356 - val_loss: 4.5914 - val_mape: 4.5914\n",
      "Epoch 246/250\n",
      " - 1s - loss: 3.8560 - mape: 3.8560 - val_loss: 3.9697 - val_mape: 3.9697\n",
      "Epoch 247/250\n",
      " - 2s - loss: 3.8679 - mape: 3.8679 - val_loss: 3.3931 - val_mape: 3.3931\n",
      "Epoch 248/250\n",
      " - 2s - loss: 3.9065 - mape: 3.9065 - val_loss: 3.0308 - val_mape: 3.0308\n",
      "Epoch 249/250\n",
      " - 1s - loss: 3.8799 - mape: 3.8799 - val_loss: 2.9006 - val_mape: 2.9006\n",
      "Epoch 250/250\n",
      " - 1s - loss: 3.8762 - mape: 3.8762 - val_loss: 3.5219 - val_mape: 3.5219\n"
     ]
    }
   ],
   "source": [
    "evolution2 = model_F.fit(X_train, y_train,\n",
    "         batch_size=10,\n",
    "         epochs=250,\n",
    "         verbose=2,\n",
    "         validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(evolution2.history['mape'])\n",
    "plt.plot(evolution2.history['val_mape'])\n",
    "plt.title(\"Model MAPE\")\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.ylabel(\"MAPE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 43us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5218935012817383"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_F.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons sur la sortie graphique que les résultats de MAPE sur les données de test fluent beaucoup. Cepandant, ils restent toujours inférieur à 10 et sont très souvent proches de la valeur 3.5 en MAPE.\n",
    "\n",
    "**Ainsi, notre modèle le plus performant est à 3.52 de MAPE\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
